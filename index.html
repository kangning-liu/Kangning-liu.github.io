<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-3GX2W3632C"></script>
    <script>window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', 'G-3GX2W3632C');</script>
    <!-- Google Tag Manager -->
    <script>(function (w, d, s, l, i) {
        w[l] = w[l] || [];
        w[l].push({
            'gtm.start':
                new Date().getTime(), event: 'gtm.js'
        });
        var f = d.getElementsByTagName(s)[0],
            j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : '';
        j.async = true;
        j.src =
            'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
        f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-W6MLN9P');</script>
    <!-- End Google Tag Manager -->
    <!--    <link rel="stylesheet" href="./Kangning Liu_files/cdnjs.cloudflare.com_ajax_libs_font-awesome_6.0.0-beta3_css_all.min.css">-->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">


    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <style>

        html {
            /* Prevent in-page anchor targets from being hidden under the sticky navbar */
            scroll-padding-top: 90px;
        }

        @media (prefers-reduced-motion: reduce) {
            html {
                scroll-behavior: auto;
            }

            *, *::before, *::after {
                animation-duration: 0.01ms !important;
                animation-iteration-count: 1 !important;
                transition-duration: 0.01ms !important;
                scroll-behavior: auto !important;
            }
        }

        /* Reset */
        body, h1, h2, h3, p {
            margin: 0;
            padding: 0;
        }

        body {
            font-family: 'Merriweather', serif;
            color: #333;
            font-size: 17px;
            line-height: 1.8;
            background-color: #fcfcfc;
        }

        h1, h2, h3, h4, h5, h6 {
            font-family: 'Lato', sans-serif;
            font-weight: 700;
            color: #2c3e50;
            margin-bottom: 1rem;
        }

        /* Basic Styles */
        .navbar {
            background-color: #2c3e50;
            color: white;
            font-family: 'Lato', sans-serif;
            border: none;
            border-radius: 0;
            margin-bottom: 40px;
            position: sticky;
            top: 0;
            z-index: 1030;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            padding: 0.5rem 1rem;
        }

        /* Style for navbar links */
        .navbar-nav .nav-item {
            list-style-type: none;
        }

        .navbar-nav .nav-link {
            color: #ecf0f1 !important;
            padding: 15px !important;
            text-decoration: none;
            display: inline-block;
            font-weight: 400;
            letter-spacing: 0.5px;
            transition: color 0.3s ease;
        }

        /* On Hover */
        .navbar-nav .nav-link:hover {
            background-color: transparent;
            color: #3498db !important;
        }

        /* Container */
        .container {
            max-width: 1100px;
            margin: auto;
            padding: 0 20px;
        }


        /* Masonry Gallery Layout - Reverted to Scroll with Filters */
        .imgGallery-row {
            display: flex;
            overflow-x: auto;
            gap: 15px;
            padding-bottom: 15px; /* Space for scrollbar */
            scroll-behavior: smooth;
            -webkit-overflow-scrolling: touch; /* Smooth scrolling on iOS */
            scrollbar-width: thin; /* Firefox */
        }

        /* Custom Scrollbar */
        .imgGallery-row::-webkit-scrollbar {
            height: 8px;
        }

        .imgGallery-row::-webkit-scrollbar-track {
            background: #f1f1f1;
            border-radius: 4px;
        }

        .imgGallery-row::-webkit-scrollbar-thumb {
            background: #ccc;
            border-radius: 4px;
        }

        .imgGallery-row::-webkit-scrollbar-thumb:hover {
            background: #aaa;
        }

        .imgGallery-column {
            flex: 0 0 300px; /* Fixed width for each image card */
            max-width: 300px;
            padding: 0;
            box-sizing: border-box;
            display: block; /* Default to block for visibility */
        }

        .imgGallery-column img {
            width: 100%;
            height: 200px; /* Fixed height for uniformity */
            object-fit: cover; /* Crop image to fit */
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
            transition: transform .3s ease-in-out, box-shadow .3s ease;
            display: block;
        }

        .imgGallery-column img:hover {
            transform: scale(1.02);
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.15);
            cursor: pointer;
        }
        
        /* Photo Filter Buttons */
        .photo-filter-container {
            text-align: center;
            margin-bottom: 20px;
        }
        
        .photo-filter-btn {
            display: inline-block;
            padding: 10px 22px;
            margin: 4px;
            font-family: 'Lato', sans-serif;
            font-size: 1.3rem;
            font-weight: 700;
            color: #7f8c8d;
            background-color: white;
            border: 1px solid #ecf0f1;
            border-radius: 30px;
            cursor: pointer;
            transition: all 0.2s ease;
        }

        .photo-filter-btn:hover {
            background-color: #f9f9f9;
            color: #3498db;
            border-color: #3498db;
        }

        .photo-filter-btn.active {
            background-color: #3498db;
            color: white;
            border-color: #3498db;
        }
        
        /* Back to Top Button */
        #backToTop {
            display: none;
            position: fixed;
            bottom: 30px;
            right: 30px;
            z-index: 99;
            border: none;
            outline: none;
            background-color: #3498db;
            color: white;
            cursor: pointer;
            padding: 15px;
            border-radius: 50%;
            box-shadow: 0 4px 10px rgba(0,0,0,0.2);
            transition: background-color 0.3s, transform 0.3s;
            width: 50px;
            height: 50px;
            text-align: center;
            line-height: 20px;
        }
        
        #backToTop:hover {
            background-color: #2980b9;
            transform: translateY(-3px);
        }

        .imgGallery-section-title {
            font-family: 'Lato', sans-serif;
            font-size: 1.3rem;
            font-weight: bold;
            text-align: left;
            color: #7f8c8d;
            margin-top: 20px;
            border-bottom: 2px solid #eee;
            padding-bottom: 5px;
            margin-bottom: 15px;
        }

        .company-header {
            display: flex;
            align-items: center;
            gap: 20px;
            margin-bottom: 10px;
        }

        .company-header img {
            width: 50px;
            height: auto;
            object-fit: contain;
        }

        .company-title {
            margin: 0;
            font-family: 'Lato', sans-serif;
            font-size: 1.3em;
            font-weight: 700;
            color: #2c3e50;
        }

        .location-time {
            font-family: 'Lato', sans-serif;
            color: #7f8c8d;
            font-size: 1.5rem;
            margin-bottom: 5px;
        }
        
        .position-advisors {
            font-family: 'Merriweather', serif;
            font-weight: bold;
            color: #34495e;
            margin-bottom: 15px;
        }

        .project-list {
            font-family: 'Merriweather', serif;
            margin-top: 15px;
            text-align: justify;
            padding-left: 20px;
        }
        
        .project-list li {
            margin-bottom: 12px;
        }

        .publication-img {
            border-radius: 8px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.08);
            margin-right: 25px; 
            transition: transform 0.2s;
        }
        
        .publication-img:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.12);
        }


        .justified-text {
            text-align: justify;
        }
        
        /* Link Styles */
        a {
            color: #2980b9;
            text-decoration: none;
            transition: color 0.2s;
        }
        
        a:hover {
            color: #c0392b;
            text-decoration: none;
        }

        /* Filter Buttons */
        .filter-container {
            text-align: center;
            margin-bottom: 30px;
        }

        .filter-btn {
            display: inline-block;
            padding: 11px 26px;
            margin: 6px;
            font-family: 'Lato', sans-serif;
            font-size: 1.3rem;
            font-weight: 700;
            color: #7f8c8d;
            background-color: white;
            border: 2px solid #ecf0f1; /* Slightly thicker border */
            border-radius: 30px; /* More rounded pill */
            cursor: pointer;
            transition: all 0.2s ease;
        }

        .filter-btn:hover {
            background-color: #f9f9f9;
            color: #3498db;
            border-color: #3498db;
            transform: translateY(-1px);
        }

        .filter-btn.active {
            background-color: #3498db;
            color: white;
            border-color: #3498db;
            box-shadow: 0 4px 10px rgba(52, 152, 219, 0.3);
        }

        /* Button/Link Badges */
        .pub-link {
            display: inline-block;
            padding: 8px 18px;
            margin: 5px 5px 5px 0;
            font-family: 'Lato', sans-serif;
            font-size: 1.3rem;
            font-weight: 600;
            color: #3498db;
            background-color: transparent;
            border: 1px solid #3498db;
            border-radius: 20px; /* Pill shape */
            text-decoration: none;
            transition: all 0.2s ease;
        }

        .pub-link:hover {
            background-color: #3498db;
            color: white !important;
            text-decoration: none;
            box-shadow: 0 2px 5px rgba(52, 152, 219, 0.3);
            transform: translateY(-1px);
        }
        
        .pub-link.disabled {
            color: #95a5a6;
            border-color: #bdc3c7;
            cursor: default;
            pointer-events: none;
        }

        .pub-link.disabled:hover {
            background-color: transparent;
            color: #95a5a6 !important;
            box-shadow: none;
            transform: none;
        }
        
        .pub-link i {
            margin-right: 5px;
        }

        /* Publication text spacing */
        .pub-list .custom-col-text p {
            margin-bottom: 0.45rem;
        }

        .pub-list .custom-col-text p.justified-text {
            margin-top: 0.8rem;
        }

        /* Default (Mobile First) */
        .custom-col-img,
        .custom-col-text {
            flex: 0 0 100%;
            max-width: 100%;
        }

        /* Landscape / wider screens */
        @media screen and (orientation: landscape) {
            .custom-col-img {
                flex: 0 0 45%;
                max-width: 45%;
            }

            .custom-col-text {
                flex: 0 0 55%;
                max-width: 55%;
                padding-left: 20px;
                text-align: justify;
            }
        }

        /* Portrait */
        @media screen and (orientation: portrait) {
            .custom-col-img {
                flex: 0 0 100%;
                max-width: 100%;
                margin-bottom: 20px;
            }

            .custom-col-text {
                flex: 0 0 100%;
                max-width: 100%;
                text-align: justify;
            }
        }


        ul.vert {
            padding: 0;
            background-color: transparent;
            list-style-type: none;
        }

        ul.vert > li {
            background-color: white;
            border-radius: 12px;
            border: none;
            padding: 30px;
            margin-bottom: 30px;
            list-style-type: none;
            box-shadow: 0 4px 6px rgba(0,0,0,0.05), 0 1px 3px rgba(0,0,0,0.1);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        
        ul.vert > li:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 20px rgba(0,0,0,0.1), 0 4px 8px rgba(0,0,0,0.06);
        }

        /* Header Refinements */
        h2 {
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
            margin-top: 50px;
            margin-bottom: 30px;
            font-weight: 300;
            color: #2c3e50;
            display: inline-block;
        }

        p {
            font-family: 'Merriweather', serif;
            color: #444;
            line-height: 1.8em;
        }

        .post-meta {
            color: #999;
            font-size: 90%;
            margin: 0;
        }

        .post-category {
            margin: 0 0.1em;
            padding: 0.3em 1em;
            color: #fff;
            background: #999;
            font-size: 80%;
        }

        .post-category-design {
            background: #5aba59;
        }

        .post-category-pure {
            background: #4d85d1;
        }

        .post-category-yui {
            background: #8156a7;
        }

        .post-category-js {
            background: #df2d4f;
        }

        .post-images {
            margin: 1em 0;
        }

        .post-image-meta {
            margin-top: -3.5em;
            margin-left: 1em;
            color: #fff;
            text-shadow: 0 1px 1px #333;
        }

        .footer {
            text-align: center;
            padding: 1em 0;
        }

        .footer a {
            color: #ccc;
            font-size: 80%;
        }

        .footer .pure-menu a:hover,
        .footer .pure-menu a:focus {
            background: none;
        }

        @media (min-width: 48em) {
            .content {
                padding: 2em 3em 0;
                margin-left: 25%;
            }

            .header {
                margin: 80% 2em 0;
                text-align: right;
            }

            .sidebar {
                position: fixed;
                top: 0;
                bottom: 0;
            }

            .nav-item {
                display: block;
            }
        }


        /* Refinements for Profile and Mobile */
        .profile-name {
            font-size: 3rem;
            color: #000000;
            font-family: 'Merriweather', serif;
            line-height: 1.2;
            margin-bottom: 10px;
        }

        .profile-subtitle {
            font-size: 1.2rem;
            color: #555;
            font-family: 'Merriweather', serif;
            margin-top: 10px;
            line-height: 1.5;
        }

        @media (max-width: 768px) {
            .profile-name {
                font-size: 2.2rem;
            }
            
            .justified-text {
                text-align: left;
            }
        }

    </style>

    <!-- Bootstrap 5 CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Load your custom CSS -->
    <link rel="stylesheet" href="firstcss.css">

    <!-- Load the font from Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&family=Merriweather:wght@300;400;700&display=swap" rel="stylesheet">

    <!-- SimpleLightbox CSS -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/simplelightbox/2.14.2/simple-lightbox.min.css" rel="stylesheet">

    <title>Kangning Liu - Research Scientist | Computer Vision & AI</title>

    <meta name="description" content="Personal website of Kangning Liu, Research Scientist at Adobe. PhD from NYU Center for Data Science. Research focus: MLLMs, Vision Foundation Models, and Robust Deep Learning.">
    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body data-new-gr-c-s-check-loaded="14.1088.0" data-gr-ext-installed="">
<!-- Google Tag Manager (noscript) -->
<noscript>
    <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W6MLN9P"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
</noscript>
<!-- End Google Tag Manager (noscript) -->

<!-- Navigation -->
<nav class="navbar navbar-expand-lg navbar-dark">
    <div class="container" id="home">
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav">
                <li class="nav-item"><a class="nav-link" href="#home">Home</a></li>
                <li class="nav-item"><a class="nav-link" href="#experience">Research Experience</a></li>
                <li class="nav-item"><a class="nav-link" href="#publications">Publications</a></li>
                <li class="nav-item"><a class="nav-link" href="#teaching">Teaching</a></li>
                <li class="nav-item"><a class="nav-link" href="#service">Service</a></li>
                <li class="nav-item"><a class="nav-link" href="#photography">Miscellaneous</a></li>
            </ul>
        </div>
    </div>
</nav>


<!-- Page Content -->
<div class="container">

    <div class="row">
        <div class="col-md-4 col-sm-12 custom-col-img">
            <!--<div style="font-size: 2rem;"><b>Kangning Liu (刘康宁)</b></div><br>-->
            <div class="profile-name">
                <b>Kangning Liu (刘康宁)</b>
                <div class="profile-subtitle">
                    Research Scientist @ Adobe | MLLM & Vision Foundation Models
                </div>
            </div>
            <br>
            <img class="img-fluid" src="./Kangning Liu_files/kangning.jpeg" alt="Kangning Liu Profile Picture"
                 style="max-width:90%;height:auto;" loading="lazy">
        </div>


        <!-- Entries Column -->
        <div class="col-md-8 col-sm-12 custom-col-text">

            <!-- Main Image -->
            <!--              -->
            <!--              <div style="font-family: &#39;Oswald&#39;, sans-serif; font-size: 32px;"><b>Kangning Liu (刘康宁)</b></div><br>-->
            <!--              <p><b>kangning.liu[at]nyu.edu</b><br>-->
            <!--            </p><p>Ph.D. student at <a href="https://cds.nyu.edu/">NYU Center for Data Science</a>.-->
            <!-- Introduction -->
            <p style="margin-top:3%; text-align:justify;">
                Hello! I am Kangning Liu, currently a Research Scientist at <a href="https://www.adobe.com/">Adobe</a>. 
                I earned my Ph.D. at the <a href="https://cds.nyu.edu/">NYU Center for Data Science</a>, where I was advised by <a
                    href="https://cims.nyu.edu/~cfgranda/">Prof. Carlos Fernandez Granda</a> and <a
                    href="https://cs.nyu.edu/~kgeras/">Prof. Krzysztof J. Geras</a>. Before that, I earned my <a
                    href="https://inf.ethz.ch/studies/master/master-ds.html">M.Sc. in Data Science from ETH Zurich</a>
                and my B.E. in Electrical Engineering from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua
                University</a>.
            </p>
            <p style="margin-top:3%; text-align:justify;">
                <strong>Research Focus:</strong> My research centers on multimodal large language models, vision foundation models, and large-scale data/annotation systems—bridging cutting-edge research with real-world impact.
                <!-- I have published at top-tier venues (e.g. <strong>NeurIPS, CVPR, ICCV</strong>) and successfully transferred technology into production systems across key Adobe products including <strong>Photoshop</strong>, <strong>Lightroom</strong>, <strong>Express</strong>, and <strong>Firefly</strong>. -->
            </p>
            <p style="margin-top:3%; text-align:justify;">    
                My current work focuses on fine-grained image understanding through vision-language modeling, enabling applications from generative AI data creation and reward modeling to AI agent tool calling and creative workflows. For example, the "selectionByPrompt" feature <a href="https://blog.adobe.com/en/publish/2025/12/10/edit-photoshop-chatgpt">[Adobe Blog]</a> enables users to select things in an image using natural language with <strong>Adobe Photoshop</strong> in <a href="https://chatgpt.com/">ChatGPT</a>.
                My previous research on segmentation foundation models has been deployed in the latest selection tools in <strong>Adobe Photoshop</strong> <a href="https://www.youtube.com/watch?v=ZJdmZ10I4H4">[Media Review]</a>, remove background in <strong>Adobe Firefly</strong> <a href="https://helpx.adobe.com/firefly/web/edit-images/transform-images-using-generative-fill/replace-background.html">[Feature Summary]</a>, and segmentation enhancements in <strong>Adobe Lightroom</strong> <a href="https://helpx.adobe.com/lightroom-classic/help/whats-new/2025-3.html">[Feature Summary]</a> and <strong>Adobe Express</strong>—enabling users to isolate complex image elements with remarkable speed and precision.
            </p>

            <p style="margin-top:3%; text-align:justify;">
                During my Ph.D., I contributed to a range of research projects focused on learning under imperfect supervision. These include uncertainty-aware fine-tuning of segmentation foundation models (<a href="https://kangning-liu.github.io/SUM_website/">SUM</a>), noise-resilient
                deep segmentation (<a href="https://github.com/Kangningthu/ADELE">ADELE</a>), weakly supervised segmentation (<a href="https://github.com/nyukat/GLAM">GLAM</a>), and unsupervised/self-supervised
                learning (<a href="https://arxiv.org/abs/2210.09452">ItS2CLR</a>). Beyond this, my expertise extends to
                video analysis (<a href="https://openreview.net/pdf?id=jIIzJaMbfw">StrokeRehab</a>) and video synthesis
                (<a href="https://arxiv.org/abs/2004.06502">UVIT</a> & <a href="https://arxiv.org/abs/2304.14471">Controllable
                Face Video Synthesis</a>).
            </p>
            <!-- <p style="margin-top:3%; text-align:justify;">
            I am currently seeking research interns for the Summer of 2025. Please feel free to drop me an email if you are interested.
            </p> -->
            <!-- Internship and Job Search -->
<!--                I was a Research Intern at <a href="https://about.google">Google</a> in Mountain View during the summer-->
<!--                of 2022. During the summer of 2023, I was a Research Scientist Intern at <a-->
<!--                    href="https://www.adobe.com/">Adobe</a> in San Jose, working on advancing the segmentation-->
<!--                foundation model with more than 100 million images.-->

                <!-- Contact Info and Links -->
            <p style="margin-top:3%; text-align:justify;">
                For more details, feel free to contact me at <b>kangning.liu[at]nyu.edu</b>. You can also find me on <a
                    href="https://scholar.google.com/citations?user=F3F2qAkAAAAJ&hl=en">Google Scholar <i
                    class="fas fa-graduation-cap"></i></a> and <a
                    href="https://www.linkedin.com/in/kangning-liu-14600282/">LinkedIn <i
                    class="fab fa-linkedin"></i></a>.
            </p>


        </div>
    </div>


    <div class="col-md-12">
        <h2 id="experience">Research Experience</h2>

        <ul class="vert">
            <li>
                <div class="company-header">
                    <img src="./Kangning Liu_files/Adobe.png" alt="Adobe Logo" loading="lazy">
                    <p class="company-title">
                        <strong>Adobe</strong>
                    </p>
                </div>
                <p class="location-time">San Jose, California, USA (April 2024 - present)</p>
                <p class="position-advisors">Research Scientist</p>

                <ul class="project-list alt justified-text">
    
                    <li><strong>Generative AI & MLLMs:</strong> Enhance the grounding ability of multimodal large language models by integrating advanced reasoning capabilities and fine-grained understanding. These models enable a wide spectrum of applications, ranging from generative AI data creation and reward modeling for GenAI training to AI agent tool calling. For example, our "selectionByPrompt" feature <a href="https://blog.adobe.com/en/publish/2025/12/10/edit-photoshop-chatgpt">[Adobe Blog]</a> enables users to select things in an image using natural language with <strong>Adobe Photoshop</strong> in <a href="https://chatgpt.com/">ChatGPT</a>.</li>
                    <li><strong>Vision Foundation Model:</strong> Developed cutting-edge segmentation technologies to enhance Adobe products. 
                        Contributions include core features deployed in Adobe Photoshop’s newest selection tools<a href="https://www.youtube.com/watch?v=ZJdmZ10I4H4"> [Media Review]</a>, remove background in Adobe Firefly <a href="https://helpx.adobe.com/firefly/web/edit-images/transform-images-using-generative-fill/replace-background.html">[Feature Summary]</a>, and segmentation enhancements in Adobe Lightroom, Adobe Express.</li>
                </ul>
            </li>
        </ul>


        <ul class="vert">
            <li>
                <div class="company-header">
                    <img src="./Kangning Liu_files/Adobe.png" alt="Adobe Logo" loading="lazy">
                    <p class="company-title">
                        <strong>Adobe</strong>
                    </p>
                </div>
                <p class="location-time">San Jose, California, USA (May 2023 - Nov 2023)</p>
                <p class="position-advisors">Research Intern | Advisors: Dr. Brian Price, Dr. Jason Kuen, Dr. Yifei Fan,
                    Dr. Zijun Wei, Luis Figueroa, Markus Woodson</p>
                <ul class="project-list alt justified-text">
                    <li><strong>Foundation Model Development & Scale:</strong> Worked on segmentation foundation model 
                        research, processing <strong>millions of images</strong> via a model-in-the-loop data engine, potentially influencing the roadmap for features in Adobe products such as Photoshop and Lightroom.
                    </li>
                    <li><strong>SOTA Performance & Quality Assurance:</strong> Achieved SOTA segmentation accuracy across 
                        20+ test sets (5-point IoU: <strong>0.869</strong>, surpassing SAM's 0.828). This work (SUM) was published 
                        in <strong>NeurIPS 2024</strong> [<a href="https://kangning-liu.github.io/SUM_website/">Project Website</a>] and established scalable feedback loops for iterative model 
                        refinement and product integration.
                    </li>
                </ul>
            </li>
        </ul>


        <!-- Google -->
        <ul class="vert">
            <li>
                <div class="company-header">
                    <img src="./Kangning Liu_files/Google.png" alt="Google Logo" loading="lazy">
                    <p class="company-title">
                        <strong>Google</strong>
                    </p>
                </div>
                <p class="location-time">Mountain View, California, USA (May 2022 - Sep 2022)</p>
                <p class="position-advisors">Research Intern | Advisors: Dr. Xuhui Jia, Dr. Yu-Chuan Su, Dr. Ruijin
                    Cang</p>
                <ul class="project-list alt justified-text">
                    <li><strong>One-shot Face Video Synthesis:</strong> Developed deep learning algorithms optimized for real-time one-shot face video synthesis, targeting efficient deployment on mobile devices.
                    </li>
                    <li><strong>Quality & Control:</strong> Innovated a method enhancing geometric correspondence (7% in AKD) and overall quality (15% in AED), while incorporating a plug-in interface for flexible emotion manipulation.
                    </li>
                </ul>
            </li>
        </ul>


        <br/>

        <ul class="vert">
            <li>
                <div class="company-header">
                    <img src="./Kangning Liu_files/NYU.png" alt="CDS Logo" loading="lazy">
                    <p class="company-title">
                        <strong>Center for Data Science,
                            New York University</strong>
                    </p>

                </div>
                <p class="location-time">New York, USA (Sept 2019 - March 2024)</p>
                <p class="position-advisors">Research Assistant | Advisors: Prof. Carlos Fernandez-Granda, Prof.
                    Krzysztof J. Geras</p>
                <ul class="project-list alt justified-text">
                    <li><strong>Robust Deep Learning with Noisy Labels:</strong> Engineered robust algorithms tailored for
                        segmentation tasks, showing resilience against annotation noise. This ensures the model's
                        reliability, especially in real-world scenarios where annotation noise is prevalent. This work (ADELE) 
                        was selected for <strong>CVPR 2022 Oral Presentation</strong> 
                        [<a href="https://github.com/Kangningthu/ADELE">GitHub</a>].
                    </li>
                    <li><strong>Weakly Supervised Learning:</strong> Proposed an innovative framework for multiple instance 
                        learning, which iteratively improves instance-level features by jointly estimating latent 
                        instance-level pseudo labels, and show that it outperforms existing methods on three real-world 
                        medical datasets. This work (ItS2CLR) was published in <strong>CVPR 2023</strong> 
                        [<a href="https://github.com/Kangningthu/ItS2CLR">GitHub</a>].
                    </li>
                    <li><strong>Video Understanding:</strong> Investigated and designed novel deep learning models for video 
                        action classification and segmentation, applied for the motion primitives prediction. This work (StrokeRehab) was published in 
                        <strong>NeurIPS 2022</strong> [<a href="https://openreview.net/forum?id=jIIzJaMbfw">Paper Link</a>].
                    </li>
                    <!-- <li><strong>Inverse Problem for Time Series Data:</strong> Engineered deep learning solutions for
                        MRI parameter estimation.
                    </li> -->
                </ul>
            </li>
        </ul>

        <br/>

        <!-- ETH Zurich -->
        <ul class="vert">
            <li>
                <div class="company-header">
                    <img src="./Kangning Liu_files/cvl2.png" alt="CVL Logo" loading="lazy">
                    <p class="company-title">
                        <strong>Computer Vision Laboratory, ETH Zurich</strong>
                    </p>
                </div>
                <p class="location-time">Zurich, Switzerland (Oct 2018 - Aug 2019)</p>
                <p class="position-advisors">Research Assistant | Advisors: Prof. Luc Van Gool, Prof. Radu Timofte,
                    Prof. Shuhang Gu</p>
                <ul class="project-list alt justified-text">
                    <li><strong>Domain Adaptation:</strong> Improved cross-domain semantic segmentation methods for
                        urban scene images through GAN models, harnessing weak paired information.
                    </li>
                    <li><strong>Video and Image Synthesis:</strong> Achieved state-of-the-art performance in
                        unsupervised multimodal video translation through bidirectional recurrent neural networks. 
                    </li>
                </ul>
            </li>
        </ul>
    </div>

    <!-- Links on the Sidebar -->

    <div class="col-md-12">
        <!-- Old: <div class="col-md-8" style="height: 120vh;"> -->
        <h2 id="publications">Publications</h2>

        <p>See also <a href="https://scholar.google.com/citations?user=F3F2qAkAAAAJ&hl=en">Google Scholar</a>.</p>

        <div class="filter-container">
            <button class="filter-btn active" data-filter="all">All</button>
            <button class="filter-btn" data-filter="mllm">MLLM</button>
            <button class="filter-btn" data-filter="foundation">Vision Foundation Model</button>
            <button class="filter-btn" data-filter="imperfect">Robust Learning</button>
            <button class="filter-btn" data-filter="video-gen">Video Generation</button>
            <button class="filter-btn" data-filter="video-understanding">Video Understanding</button>
            <button class="filter-btn" data-filter="others">Others</button>
            <div id="filter-count" style="margin-top: 10px; color: #7f8c8d; font-size: 1.2rem; font-style: italic;"></div>
        </div>

        <ul class="vert pub-list">

            <li class="publication-item" data-category="mllm">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/crop_vgent_method_v2.png" alt="Publication Image"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>VGent: Visual Grounding via Modular Design for Disentangling Reasoning and Prediction</b></p>
                        <p>
                            <span class="pub-link disabled"><i class="fas fa-file-pdf"></i> Paper (Coming Soon)</span>
                        </p>
                        <p>Weitai Kang, Jason Kuen, Mengwei Ren, Zijun Wei, Yan Yan, <u><b>Kangning Liu*</b></u>. (*project lead)</p>
                        <p>[Preprint]</p>
                        <p class="justified-text">
                            Current visual grounding models either rely on auto-regressive MLLMs (slow with hallucination risks) or re-align LLMs with vision features (potentially undermining reasoning). 
                            In contrast, we propose <strong>VGent</strong>, a modular encoder-decoder architecture that explicitly disentangles high-level reasoning and low-level bounding box prediction. 
                            A frozen MLLM serves as the encoder for powerful reasoning, while a decoder selects target box(es) from detector-proposed candidates via cross-attention on encoder hidden states. 
                            This design leverages advances in both object detection and MLLMs, avoids auto-regressive pitfalls, and enables fast inference. 
                            We introduce: (i) <strong>QuadThinker</strong>, an RL-based training paradigm for multi-target reasoning; (ii) <strong>mask-aware labels</strong> for resolving detection-segmentation ambiguity; and (iii) <strong>global target recognition</strong> to improve target identification. 
                            VGent achieves state-of-the-art performance with <strong>+20.6% F1</strong> improvement, <strong>+8.2% gIoU</strong>, and <strong>+5.8% cIoU</strong> gains under visual reference challenges, while maintaining constant, fast inference.
                        </p>
                    </div>
                </div>
            </li>
            <li class="publication-item" data-category="mllm">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/LaViDa-O.png" alt="Publication Image"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>LaViDa-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation </b></p>
                        <p>
                            <a href="https://arxiv.org/abs/2509.19244" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                            <a href="https://homepage.jackli.org/projects/lavida_o/index.html" class="pub-link"><i class="fas fa-globe"></i> Project Website</a>
                        </p>
                        <p>Shufan Li, Jiuxiang Gu, <u><b>Kangning Liu</b></u>, Zhe Lin, Zijun Wei, Aditya Grover, Jason Kuen.</p>
                        <p>[Preprint]</p>
                        <p>
                            We propose <strong>LaViDa-O</strong>, a unified Masked Diffusion Model (MDM) that unifies multimodal understanding and high-resolution generation (1024px) within a single framework. 
                            By incorporating a novel <strong>Elastic Mixture-of-Transformers (Elastic-MoT)</strong> architecture, LaViDa-O couples a lightweight generation branch with a robust understanding branch, enabling efficient token compression and stratified sampling. 
                            The model further leverages planning and iterative self-reflection to boost generation quality. 
                            LaViDa-O achieves state-of-the-art performance across benchmarks for object grounding (RefCOCO), text-to-image generation (GenEval), and image editing (ImgEdit), outperforming leading autoregressive and continuous diffusion models while offering considerable inference speedups. 
                        </p>
                    </div>
                </div>
            </li>
            <li class="publication-item" data-category="mllm">
                <div class="row">
                    <!-- Publication Details -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/RAS.jpg" alt="Publication Image"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Refer to Any Segmentation Mask Group With Vision-Language Prompts </b></p>
                        <p>
                            <a href="https://arxiv.org/abs/2506.05342" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                            <a href="https://research.adobe.com/news/adobe-research-breakthroughs-at-iccv-2025/" class="pub-link"><i class="fas fa-video"></i> Featured Demo</a>
                        </p>
                        <p>Shengcao Cao, Zijun Wei, Jason Kuen, <u><b>Kangning Liu</b></u>, Lingzhi Zhang, Jiuxiang Gu, HyunJoon Jung, Liang-Yan Gui, Yu-Xiong Wang</p>
                        <p>[ICCV 2025]</p>
                        <p>
                            We introduce a novel task of omnimodal referring expression segmentation (ORES). In this task, a model produces a group of masks based on arbitrary prompts specified by text only or text plus reference visual entities. 
                            We propose a novel framework to "Refer to Any Segmentation Mask Group" (RAS), which augments segmentation models with complex multimodal interactions and comprehension via a mask-centric large multimodal model. 
                        </p>
                    </div>
                </div>
            </li>

            <li class="publication-item" data-category="foundation">
                <div class="row">
                    <!-- Publication Details -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/SVE.png" alt="Publication Image"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Leveraging Sparsity for Efficient Inference of High-Resolution Vision Foundation Models. </b></p>
                        <p>
                            <a href="https://bmva-archive.org.uk/bmvc/2025/assets/papers/Paper_661/paper.pdf" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p>Xin Xu, Jason Kuen, Brian L. Price, <u><b>Kangning Liu</b></u>, Zijun Wei, Yu-Xiong Wang.</p>
                        <p>  [BMVC 2025]</p>
                        <p class="justified-text">
                            We introduce Sparse Vision Encoder (SVE), a post-training optimization framework that exploits naturally emerging sparsity in attention maps to accelerate inference of vision encoders. SVE selectively applies sparsity in key layers, performs sparsity distillation, and leverages cross-layer consistency to reuse sparsity structures. Experiments on DINOv2, CLIP, and SAM2 demonstrate up to 80% speedup for high-resolution encoding while preserving model performance
                        </p>
                    </div>
                </div>
            </li>
            <li class="publication-item" data-category="foundation">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/conSAMme.jpg" alt="Publication Image"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>ConSAMmé: Achieving Consistent Segmentations with SAM </b></p>
                        <p>
                            <a href="https://openaccess.thecvf.com/content/CVPR2025W/NTIRE/html/Myers-Dean_conSAMme_Achieving_Consistent_Segmentations_with_SAM_CVPRW_2025_paper.html" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p>Josh Myers-Dean, <u><b>Kangning Liu</b></u>, Brian Price, Yifei Fan, Danna Gurari. </p>
                        <p>  [CVPR 2025 NTIRE Workshop.]</p>
                        <p class="justified-text">
                            Multi-output interactive segmentation methods generate multiple binary masks when given user guidance, such as clicks.
                            However, it is unpredictable whether the order of the masks will match or whether those masks will be the same when given slightly different user guidance. 
                            To address these issues, we propose conSAMmé, a contrastive learning framework that conditions on explicit hierarchical semantics and leverages weakly supervised part segmentation data and a novel episodic click sampling strategy. 
                            Evaluation of conSAMmé’s performance, click robustness, and mask ordering show substantial improvements to baselines with less than 1% extra training data compared to the amount of data used for the baseline. 
                        </p>
                    </div>
                </div>
            </li>


            <li class="publication-item" data-category="foundation imperfect">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/SUM_framework.png" alt="Publication Image"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Uncertainty-aware Fine-tuning of Segmentation Foundation Models </b></p>
                        <p>
                            <a href="https://openreview.net/pdf?id=qNXRXUC90b" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                            <a href="https://github.com/Kangningthu/SUM.git" class="pub-link"><i class="fab fa-github"></i> GitHub</a>
                            <a href="https://kangning-liu.github.io/SUM_website/" class="pub-link"><i class="fas fa-globe"></i> Project Website</a>
                        </p>
                        <p><u><b>Kangning Liu</b></u>, Brian Price, Jason Kuen, Yifei Fan, Zijun Wei, Luis Figueroa,
                            Krzysztof J. Geras, Carlos Fernandez-Granda. </p>
                        <p> [NeurIPS 2024]</p>
                        <p class="justified-text">
                            The Segment Anything Model (SAM) is a large-scale foundation model that has revolutionized
                            segmentation methodology. Despite its impressive generalization ability, the segmentation
                            accuracy of SAM on images with intricate structures is unsatisfactory in many cases. We
                            introduce the Segmentation with Uncertainty Model (SUM), which enhances the accuracy of
                            segmentation foundation models by incorporating an uncertainty-aware training loss and
                            prompt sampling based on the estimated uncertainty of pseudo-labels. We evaluated the
                            proposed SUM on a diverse test set consisting of 22 public benchmarks, where it achieves
                            state-of-the-art results. Notably, our method consistently surpasses SAM by 3-6 points in
                            mean IoU and 4-7 in mean boundary IoU across point-prompt interactive segmentation rounds.
                        </p>
                    </div>
                </div>
            </li>

            <li class="publication-item" data-category="video-gen">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/approach_v3.png" alt="Publication Image"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Controllable One-Shot Face Video Synthesis With Semantic Aware Prior </b></p>
                        <p>
                            <a href="https://arxiv.org/abs/2304.14471" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p><u><b>Kangning Liu</b></u>, Yu-Chuan Su, Wei(Alex) Hong, Ruijin Cang, Xuhui Jia.</p>
                        <p>[Preprint]</p>
                        <p>
                            We propose a method that leverages rich face prior information to generate face videos with
                            improved semantic consistency and expression preservation.
                            Our model improves the baseline by 7% in average keypoint distance and outperforms the
                            baseline by 15% in average emotion embedding distance, while also providing a convenient
                            interface for highly controllable generation in terms of pose and expression.
                        </p>
                    </div>
                </div>
            </li>

            <li class="publication-item" data-category="imperfect">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/contrastive_learning_illustration.png" alt="Publication Image"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>

                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Multiple Instance Learning via Iterative Self-Paced Supervised Contrastive Learning</b></p>
                        <p>
                            <a href="https://arxiv.org/abs/2210.09452" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                            <a href="https://github.com/Kangningthu/ItS2CLR" class="pub-link"><i class="fab fa-github"></i> GitHub</a>
                        </p>
                        <p><u><b>Kangning Liu*</b></u>,Weicheng Zhu* <u>(*Equal contribution)</u>, Yiqiu Shen, Sheng
                            Liu, Narges Razavian, Krzysztof J. Geras, Carlos Fernandez-Granda. </p>
                        <p> [CVPR 2023]</p>
                        <p class="justified-text">In this paper, we introduce Iterative Self-Paced Supervised Contrastive Learning (Its2CLR), a
                            novel method for learning high-quality instance-level representations in Multiple Instance
                            Learning (MIL). Key features of our method: 1) self-paced learning to handle label noise and
                            uncertainty; 2) supervised contrastive learning to learn discriminative instance-level
                            embeddings; 3) iterative refinement of instance labels for robust and accurate
                            classification.</p>

                    </div>
                </div>
            </li>
            <li class="publication-item" data-category="imperfect">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/ADELE.jpeg" alt="Publication Image"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Adaptive early-learning correction for segmentation from noisy annotations </b></p>
                        <p>
                            <a href="https://arxiv.org/abs/2110.03740" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                            <a href="https://github.com/Kangningthu/ADELE" class="pub-link"><i class="fab fa-github"></i> GitHub</a>
                        </p>
                        <p>Sheng Liu*, <u><b>Kangning Liu*</b></u><u> (*Equal contribution, order decided by coin
                            flip.)</u>, Weicheng Zhu, Yiqiu Shen, Carlos Fernandez-Granda. </p>
                        <p> [CVPR 2022. <font color="#c0392b">Oral 4.2% acceptance rate</font>]</p>
                        <p class="justified-text">Deep learning in the presence of noisy annotations has been studied
                            extensively in classification, but much less in segmentation tasks. In this project, we
                            study the learning dynamics of deep segmentation networks trained on inaccurately-annotated
                            data and propose a new method for semantic segmentation ADaptive Early-Learning corrEction
                            (ADELE).</p>
                    </div>
                </div>
            </li>
            <li class="publication-item" data-category="video-understanding">
                <div class="row">
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/Strokerehab.jpeg" alt="Publication Image for StrokeRehab"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                        <img src="./Kangning Liu_files/StrokeRehab2.jpeg" alt="Publication Image for StrokeRehab"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>StrokeRehab: A Benchmark Dataset for Sub-second Action Identification </b></p>
                        <p>
                            <a href="https://openreview.net/pdf?id=jIIzJaMbfw" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p>Aakash Kaku*, <u><b>Kangning Liu*</b></u> <u>(*Equal contribution)</u>, Avinash Parnandi*,
                            Haresh Rengaraj Rajamohan, Anita Venkatesan, Audre Wirtanen, Natasha Pandit, Kannan
                            Venkataramanan, Heidi Schambra, Carlos Fernandez-Granda.</p>
                        <p>[NeurIPS 2022]</p>
                        <p class="justified-text">We introduce a new benchmark dataset for the identification of subtle
                            and short-duration actions. We also propose a novel seq2seq approach, which outperforms the
                            existing methods on the new as well as standard benchmark datasets.</p>
                    </div>
                </div>
            </li>

            <li class="publication-item" data-category="others">
                <div class="row">
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/Equal.jpeg" alt="Publication Image for Neural Collapse"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>

                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Are All Losses Created Equal: A Neural Collapse Perspective </b></p>
                        <p>
                            <a href="https://openreview.net/pdf?id=8rfYWE3nyXl" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p>Jinxin Zhou, Chong You, Xiao Li, <u><b>Kangning Liu</b></u>, Sheng Liu, Qing Qu, Zhihui Zhu.
                        </p>
                        <p>[NeurIPS 2022]</p>
                        <p class="justified-text">A broad family of loss functions leads to neural collapse solutions
                            hence are equivalent on training set; moreover, they exhibit largely identical performance
                            on test data as well.</p>
                    </div>
                </div>
            </li>

            <!-- Start of the paper: Cramér-Rao bound-informed training of neural networks for quantitative MRI -->
            <li class="publication-item" data-category="others">
                <div class="row">
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/MRF.jpeg" alt="Publication Image for Cramér-Rao"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Cramér-Rao bound-informed training of neural networks for quantitative MRI </b></p>
                        <p>
                            <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.29206" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p>Xiaoxia Zhang*, Quentin Duchemin*, <u><b>Kangning Liu*</b></u> <u>(*Equal contribution)</u>,
                            Sebastian Flassbeck, Cem Gultekin, Carlos Fernandez-Granda, Jakob Asslander.</p>
                        <p>[Magnetic Resonance in Medicine 2021]</p>
                        <p class="justified-text">To address the parameter estimation problem in heterogeneous parameter
                            spaces, we propose a theoretically well-founded loss function based on the Cramér-Rao bound
                            (CRB), which provides a theoretical lower bound for the variance of an unbiased
                            estimator.</p>
                    </div>
                </div>
            </li>
            <!-- End of the paper -->
            <!-- Start of the paper: Weakly-supervised High-resolution Segmentation of Mammography Images -->
            <li class="publication-item" data-category="imperfect">
                <div class="row">
                    <div class="custom-col-img" style="padding: 10px;">
                        <!--<img src="./Kangning Liu_files/Inference_framework.png" alt="Publication Image for Weakly-supervised High-resolution Segmentation" class="img-fluid publication-img" style="width: 100%;" loading="lazy">-->
                        <img src="./Kangning Liu_files/Visualization_example.png"
                             alt="Publication Image for Weakly-supervised High-resolution Segmentation"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Weakly-supervised High-resolution Segmentation of Mammography Images for Breast Cancer Diagnosis </b></p>
                        <p>
                            <a href="https://arxiv.org/abs/2106.07049" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                            <a href="https://github.com/nyukat/GLAM" class="pub-link"><i class="fab fa-github"></i> GitHub</a>
                        </p>
                        <p><u><b>Kangning Liu</b></u>, Yiqiu Shen, Nan Wu, Jakub Piotr Chdowski, Carlos
                            Fernandez-Granda, and Krzysztof J. Geras</p>
                        <p>[Medical Imaging with Deep Learning 2021]</p>
                        <p class="justified-text">
                            In this study, we unveil a new neural network model for weakly-supervised, high-resolution
                            image segmentation.
                            Focused on breast cancer diagnosis via mammography, our method first identifies regions of
                            interest and then segments them in detail.
                            Validated on a substantial, clinically-relevant dataset, our approach significantly
                            outperforms existing methods, improving lesion localization performance by up to 39.6% and
                            20.0% for benign and malignant cases, respectively.
                        </p>
                    </div>
                </div>
            </li>
            <!-- End of the paper -->
            <!-- Start of the paper: Unsupervised Multimodal Video-to-Video Translation -->
            <li class="publication-item" data-category="video-gen">
                <div class="row">
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/UVIT_HR_multimodal.jpeg"
                             alt="Publication Image for Unsupervised Multimodal Video-to-Video Translation"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                        <!--<img src="./Kangning Liu_files/UVIT_overview.png" alt="Publication Image for Unsupervised Multimodal Video-to-Video Translation" class="img-fluid publication-img" style="width: 100%;" loading="lazy">-->
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Unsupervised Multimodal Video-to-Video Translation via Self-Supervised Learning</b></p>
                        <p>
                            <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Liu_Unsupervised_Multimodal_Video-to-Video_Translation_via_Self-Supervised_Learning_WACV_2021_paper.pdf" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                            <a href="https://uvit.netlify.app" class="pub-link"><i class="fas fa-globe"></i> Project Website</a>
                        </p>
                        <p><u><b>Kangning Liu*</b></u>, Shuhang Gu*<u>(*Equal contribution)</u>, Andrs Romero, and Radu
                            Timofte</p>
                        <p>[WACV 2021]</p>
                        <p class="justified-text">
                            In this project, we introduce an unsupervised video-to-video translation model that
                            decouples style and content.
                            Leveraging specialized encoder-decoder and bidirectional RNNs, our model excels in
                            propagating inter-frame details.
                            This architecture enables style-consistent translations and offers a user-friendly interface
                            for cross-modality conversion.
                            We also implement a self-supervised training approach using a novel video interpolation loss
                            that captures sequence-based temporal information.
                        </p>
                    </div>
                </div>
            </li>
            <!-- End of the paper -->
            <!-- Start of the paper: An interpretable classifier for high-resolution breast cancer screening images -->
            <li class="publication-item" data-category="imperfect">
                <div class="row">
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/gmic.jpeg"
                             alt="Publication Image for An interpretable classifier for high-resolution breast cancer screening images"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>An interpretable classifier for high-resolution breast cancer screening images </b></p>
                        <p>
                            <a href="https://www.sciencedirect.com/science/article/pii/S1361841520302723" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p>Shen, Yiqiu, Nan Wu, Jason Phang, Jungkyu Park, <u><b>Kangning Liu</b></u>, Sudarshini Tyagi,
                            Laura Heacock et al.</p>
                        <p>[Medical image analysis, 2021]</p>
                        <p class="justified-text">
                            Medical images differ from natural images in significantly higher resolutions and smaller
                            regions of interest.
                            Because of these differences, neural network architectures that work well for natural images
                            might not be applicable to medical image analysis.
                            In this work, we propose a novel neural network model to address these unique properties of
                            medical images.
                        </p>
                    </div>
                </div>
            </li>
            <!-- End of the paper -->

        </ul>

    </div>

    <div class="col-md-12">
        <h2 id="teaching">Teaching</h2>
        <ul class="vert">
            <li>
                <p><b>Probability and Statistics for Data Science, NYU, Center for Data Science</b></p>
                <p>Teaching Assistant (Sept - Dec 2021)</p>
            </li>
            <li>
                <p><b>Advanced Machine Learning, ETH Zurich, Department of Computer Science</b></p>
                <p>Teaching Assistant (Sept - Dec 2018)</p>
            </li>
        </ul>

    </div>


    <div class="col-md-12">
        <h2 id="service">Service</h2>
        <ul class="vert">
            <li>
                <p><b>Award</b></p>
                <p> <a href="https://neurips.cc/Conferences/2024/ProgramCommittee#top-reviewers">Top Reviewer</a> for NeurIPS 2024</p>
            </li>
            <li>
<!--                <p>Conference Reviewer for CVPR (2022, 2023, 2024), ICCV (2023), ECCV (2024), NeurIPS (2022, 2023, 2024), ICLR-->
<!--                    (2025), AISTATS (2024, 2025), AAAI (2023, 2024, 2025), WACV (2021, 2022, 2023) and MIDL (2022,-->
<!--                    2023) </p>-->
                <p><b>Conference Reviewer for</b></p>
                <p>CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML, AISTATS, AAAI, WACV</p>
            </li>
            <li>
                <p><b>Journal Reviewer for</b></p>
                <p>TPAMI,TMLR, TNNLS, CVIU</p>
            </li>
        </ul>

    </div>


    <h2 id="photography">Miscellaneous</h2>

    <ul class="vert">
        <li>
            <p>Capturing moments and exploring light with my Sony A7M4.</p>
        </li>
    </ul>
    <br/>
    <div class="col-md-12">
        <p class="imgGallery-section-title">Visual Fragments</p>
        
        <div class="photo-filter-container">
            <button class="photo-filter-btn active" data-filter="all">All</button>
            <button class="photo-filter-btn" data-filter="california">California Style</button>
            <button class="photo-filter-btn" data-filter="nyc">NYC Moments</button>
            <button class="photo-filter-btn" data-filter="night">Night and Light</button>
            <button class="photo-filter-btn" data-filter="industrial">Industrial</button>
            <button class="photo-filter-btn" data-filter="nature">Nature & Travels</button>
        </div>

        <div class="imgGallery-row">
            <!-- California -->
            <div class="imgGallery-column" data-category="california">
                <a href="./Kangning Liu_files/DSC04444.jpeg" class="lightbox-gallery" title="California">
                    <img src="./Kangning Liu_files/DSC04444.jpeg" alt="California" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="california">
                <a href="./Kangning Liu_files/DSC04257.jpeg" class="lightbox-gallery" title="California">
                    <img src="./Kangning Liu_files/DSC04257.jpeg" alt="California" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="california">
                <a href="./Kangning Liu_files/DSC04158-2.jpeg" class="lightbox-gallery" title="California">
                    <img src="./Kangning Liu_files/DSC04158-2.jpeg" alt="California" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="california">
                <a href="./Kangning Liu_files/DSC06188.jpeg" class="lightbox-gallery" title="California">
                    <img src="./Kangning Liu_files/DSC06188.jpeg" alt="California" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="california">
                <a href="./Kangning Liu_files/DSC03609.jpeg" class="lightbox-gallery" title="California">
                    <img src="./Kangning Liu_files/DSC03609.jpeg" alt="California" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="california">
                <a href="./Kangning Liu_files/DSC07554-2.jpeg" class="lightbox-gallery" title="California">
                    <img src="./Kangning Liu_files/DSC07554-2.jpeg" alt="California" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="california">
                <a href="./Kangning Liu_files/DSC08122-2.jpeg" class="lightbox-gallery" title="California">
                    <img src="./Kangning Liu_files/DSC08122-2.jpeg" alt="California" loading="lazy">
                </a>
            </div>

            <!-- NYC -->
            <div class="imgGallery-column" data-category="nyc">
                <a href="./Kangning Liu_files/8V4B0383.jpg" class="lightbox-gallery" title="NYC">
                    <img src="./Kangning Liu_files/8V4B0383.jpg" alt="NYC" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nyc">
                <a href="./Kangning Liu_files/8V4B0444-2.jpg" class="lightbox-gallery" title="NYC">
                    <img src="./Kangning Liu_files/8V4B0444-2.jpg" alt="NYC" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nyc">
                <a href="./Kangning Liu_files/DSC04963-4.jpg" class="lightbox-gallery" title="NYC">
                    <img src="./Kangning Liu_files/DSC04963-4.jpg" alt="NYC" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nyc">
                <a href="./Kangning Liu_files/DSC05142.jpeg" class="lightbox-gallery" title="NYC">
                    <img src="./Kangning Liu_files/DSC05142.jpeg" alt="NYC" loading="lazy">
                </a>
            </div> 
            <div class="imgGallery-column" data-category="nyc">
                <a href="./Kangning Liu_files/DSC07398.jpg" class="lightbox-gallery" title="NYC">
                    <img src="./Kangning Liu_files/DSC07398.jpg" alt="NYC" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nyc">
                <a href="./Kangning Liu_files/DSC07301.jpg" class="lightbox-gallery" title="NYC">
                    <img src="./Kangning Liu_files/DSC07301.jpg" alt="NYC" loading="lazy">
                </a>
            </div> 
            <div class="imgGallery-column" data-category="nyc">
                <a href="./Kangning Liu_files/DSC08667.jpg" class="lightbox-gallery" title="NYC">
                    <img src="./Kangning Liu_files/DSC08667.jpg" alt="NYC" loading="lazy">
                </a>
            </div>

            <!-- Industrial & Night -->
            <div class="imgGallery-column" data-category="night">
                <a href="./Kangning Liu_files/DSC09256.jpeg" class="lightbox-gallery" title="Night">
                    <img src="./Kangning Liu_files/DSC09256.jpeg" alt="Night" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="night">
                <a href="./Kangning Liu_files/DSC09385.jpeg" class="lightbox-gallery" title="Night">
                    <img src="./Kangning Liu_files/DSC09385.jpeg" alt="Night" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="night">
                <a href="./Kangning Liu_files/DSC09265.jpeg" class="lightbox-gallery" title="Night">
                    <img src="./Kangning Liu_files/DSC09265.jpeg" alt="Night" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="industrial">
                <a href="./Kangning Liu_files/DSC06520-3.jpeg" class="lightbox-gallery" title="SteelRacks">
                    <img src="./Kangning Liu_files/DSC06520-3.jpeg" alt="SteelRacks" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="industrial">
                <a href="./Kangning Liu_files/DSC06601-2.jpeg" class="lightbox-gallery" title="SteelRacks">
                    <img src="./Kangning Liu_files/DSC06601-2.jpeg" alt="SteelRacks" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="industrial">
                <a href="./Kangning Liu_files/DSC06482-4.jpeg" class="lightbox-gallery" title="SteelRacks">
                    <img src="./Kangning Liu_files/DSC06482-4.jpeg" alt="SteelRacks" loading="lazy">
                </a>
            </div>

            <!-- Nature & Travels -->
            <div class="imgGallery-column" data-category="nature">
                <a href="./Kangning Liu_files/DSC05394.jpg" class="lightbox-gallery" title="Vermont">
                    <img src="./Kangning Liu_files/DSC05394.jpg" alt="Vermont" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nature">
                <a href="./Kangning Liu_files/DSC05554-2.jpg" class="lightbox-gallery" title="Vermont">
                    <img src="./Kangning Liu_files/DSC05554-2.jpg" alt="Vermont" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nature">
                <a href="./Kangning Liu_files/DSC05782.jpg" class="lightbox-gallery" title="Vermont">
                    <img src="./Kangning Liu_files/DSC05782.jpg" alt="Vermont" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nature">
                <a href="./Kangning Liu_files/DSC05962.jpg" class="lightbox-gallery" title="Oregon">
                    <img src="./Kangning Liu_files/DSC05962.jpg" alt="Oregon" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nature">
                <a href="./Kangning Liu_files/DSC05801.jpg" class="lightbox-gallery" title="Oregon">
                    <img src="./Kangning Liu_files/DSC05801.jpg" alt="Oregon" loading="lazy">
                </a>
            </div>
        </div>
    </div>


    <!-- /.container -->
    <!-- Other people may like it too! -->
    <!--    <a style="color:#b5bec9;font-size:0.8em; float:right;" href="https://github.com/mavroudisv/plain-academic">Plain Academic</a> -->

    <!-- <br/>

    <br/>
    <ul class="vert">
        <li>
            <p>Pageviews</p>
        </li>
    </ul> -->
    <p>Pageviews</p>
    <script type="text/javascript" id="clustrmaps"
            src="//clustrmaps.com/map_v2.js?d=nk2suWeO4QvLQb3OSDKbWMWpp_rc2lrGUiGq_rrnFQg&cl=ffffff&w=a"></script>
</div>

<!-- Back to Top Button -->
<button id="backToTop" title="Go to top"><i class="fas fa-arrow-up"></i></button>

<!-- Bootstrap 5 JS Bundle -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
<!-- jQuery (required for custom script) -->
<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/simplelightbox/2.14.2/simple-lightbox.min.js"></script>

    <script>
    $(document).ready(function() {
        // Button accessibility: mirror "active" visual state with aria-pressed
        function syncAriaPressed($buttons) {
            $buttons.each(function() {
                var isActive = $(this).hasClass('active');
                $(this).attr('aria-pressed', isActive ? 'true' : 'false');
            });
        }

        // Publication Filter Logic with Counter
        function updateFilterCount() {
            var visibleCount = $('.publication-item:visible').length;
            var totalCount = $('.publication-item').length;
            $('#filter-count').text(`Showing ${visibleCount} of ${totalCount} papers`);
        }

        $('.filter-btn').click(function() {
            var filterValue = $(this).attr('data-filter');
            
            // Update active button
            $('.filter-btn').removeClass('active');
            $(this).addClass('active');
            syncAriaPressed($('.filter-btn'));
            
            // Filter items
            if(filterValue === 'all') {
                $('.publication-item').fadeIn('fast').promise().done(function() {
                    updateFilterCount();
                });
            } else {
                $('.publication-item').each(function() {
                    var itemCategories = $(this).attr('data-category');
                    if(itemCategories && itemCategories.includes(filterValue)) {
                        $(this).fadeIn('fast');
                    } else {
                        $(this).fadeOut('fast');
                    }
                }).promise().done(function() {
                    updateFilterCount();
                });
            }
        });

        // Initialize Publication Counter
        updateFilterCount();
        syncAriaPressed($('.filter-btn'));

        // Photography Filter Logic
        $('.photo-filter-btn').click(function() {
            var filterValue = $(this).attr('data-filter');
            
            // Update active button
            $('.photo-filter-btn').removeClass('active');
            $(this).addClass('active');
            syncAriaPressed($('.photo-filter-btn'));
            
            // Filter items
            if(filterValue === 'all') {
                $('.imgGallery-column').fadeIn('fast');
            } else {
                $('.imgGallery-column').each(function() {
                    var itemCategory = $(this).attr('data-category');
                    if(itemCategory === filterValue) {
                        $(this).fadeIn('fast');
                    } else {
                        $(this).fadeOut('fast');
                    }
                });
            }
        });
        syncAriaPressed($('.photo-filter-btn'));

        // Lightbox Initialization
        var lightbox = new SimpleLightbox('.imgGallery-row a.lightbox-gallery', { 
            captionsData: 'alt',
            captionDelay: 250,
            animationSpeed: 250,
            fadeSpeed: 200
        });
        
        // Refresh Lightbox on filter change
        $('.photo-filter-btn').on('click', function() {
             setTimeout(function(){
                 lightbox.refresh();
             }, 300); 
        });

        // Back to Top Button
        var btn = $('#backToTop');

        $(window).scroll(function() {
            if ($(window).scrollTop() > 300) {
                btn.fadeIn();
            } else {
                btn.fadeOut();
            }
        });

        btn.on('click', function(e) {
            e.preventDefault();
            $('html, body').animate({scrollTop:0}, '300');
        });
    });
</script>

</body>
<grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration>
</html>

