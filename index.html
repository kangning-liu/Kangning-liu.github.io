<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-3GX2W3632C"></script>
    <script>window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', 'G-3GX2W3632C');</script>
    <!-- Google Tag Manager -->
    <script>(function (w, d, s, l, i) {
        w[l] = w[l] || [];
        w[l].push({
            'gtm.start':
                new Date().getTime(), event: 'gtm.js'
        });
        var f = d.getElementsByTagName(s)[0],
            j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : '';
        j.async = true;
        j.src =
            'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
        f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-W6MLN9P');</script>
    <!-- End Google Tag Manager -->
    <!--    <link rel="stylesheet" href="./Kangning Liu_files/cdnjs.cloudflare.com_ajax_libs_font-awesome_6.0.0-beta3_css_all.min.css">-->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">


    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <!-- Bootstrap 5 CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Load your custom CSS -->
    <link rel="stylesheet" href="main.css">

    <!-- Load the font from Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&family=Merriweather:wght@300;400;700&display=swap" rel="stylesheet">

    <!-- SimpleLightbox CSS -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/simplelightbox/2.14.2/simple-lightbox.min.css" rel="stylesheet">

    <title>Kangning Liu - Research Scientist | Computer Vision & AI</title>

    <!-- JSON-LD Person Schema -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "Person",
        "name": "Kangning Liu",
        "alternateName": "刘康宁",
        "url": "https://kangning-liu.github.io/",
        "image": "https://kangning-liu.github.io/Kangning Liu_files/kangning.jpeg",
        "jobTitle": "Research Scientist",
        "worksFor": {
            "@type": "Organization",
            "name": "Adobe",
            "url": "https://www.adobe.com/"
        },
        "alumniOf": [
            {
                "@type": "CollegeOrUniversity",
                "name": "New York University",
                "department": "Center for Data Science"
            },
            {
                "@type": "CollegeOrUniversity",
                "name": "ETH Zurich"
            },
            {
                "@type": "CollegeOrUniversity",
                "name": "Tsinghua University"
            }
        ],
        "knowsAbout": ["Multimodal Large Language Models", "Vision Foundation Models", "Computer Vision", "Deep Learning", "Image Segmentation"],
        "sameAs": [
            "https://scholar.google.com/citations?user=F3F2qAkAAAAJ&hl=en",
            "https://github.com/Kangningthu",
            "https://www.linkedin.com/in/kangning-liu-14600282/"
        ]
    }
    </script>

    <meta name="description" content="Personal website of Kangning Liu, Research Scientist at Adobe. PhD from NYU Center for Data Science. Research focus: MLLMs, Vision Foundation Models, and Robust Deep Learning.">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Open Graph / Social Sharing -->
    <meta property="og:title" content="Kangning Liu - Research Scientist | Computer Vision & AI">
    <meta property="og:description" content="Research Scientist at Adobe. PhD from NYU. Research focus: Multimodal Large Language Models, Vision Foundation Models, and Robust Deep Learning.">
    <meta property="og:image" content="https://kangning-liu.github.io/Kangning Liu_files/kangning.jpeg">
    <meta property="og:url" content="https://kangning-liu.github.io/">
    <meta property="og:type" content="website">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Kangning Liu - Research Scientist | Computer Vision & AI">
    <meta name="twitter:description" content="Research Scientist at Adobe. PhD from NYU. Research focus: MLLMs, Vision Foundation Models, and Robust Deep Learning.">
    <meta name="twitter:image" content="https://kangning-liu.github.io/Kangning Liu_files/kangning.jpeg">
</head>
<body>
<!-- Google Tag Manager (noscript) -->
<noscript>
    <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W6MLN9P"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
</noscript>
<!-- End Google Tag Manager (noscript) -->

<!-- Navigation -->
<nav class="navbar navbar-expand-lg navbar-dark">
    <div class="container">
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav">
                <li class="nav-item"><a class="nav-link" href="#home">Home</a></li>
                <li class="nav-item"><a class="nav-link" href="#experience">Research Experience</a></li>
                <li class="nav-item"><a class="nav-link" href="#publications">Publications</a></li>
                <li class="nav-item"><a class="nav-link" href="#teaching">Teaching</a></li>
                <li class="nav-item"><a class="nav-link" href="#service">Service</a></li>
                <li class="nav-item"><a class="nav-link" href="#photography">Miscellaneous</a></li>
            </ul>
            <ul class="navbar-nav ms-auto">
                <li class="nav-item">
                    <button id="theme-toggle" class="nav-link" style="background: none; border: none; cursor: pointer; padding: 15px !important;" aria-label="Toggle dark mode">
                        <i class="fas fa-moon" id="theme-icon"></i>
                    </button>
                </li>
            </ul>
        </div>
    </div>
</nav>


<!-- Page Content -->
<main class="container" id="home">

    <section class="row align-items-center" id="about" style="margin-bottom: 50px;">
        <div class="col-md-4 col-sm-12" style="text-align: center;">
            <!--<div style="font-size: 2rem;"><b>Kangning Liu (刘康宁)</b></div><br>-->
            <div class="profile-name">
                <b>Kangning Liu (刘康宁)</b>
                <div class="profile-subtitle">
                    Research Scientist @ Adobe | MLLM & Vision Foundation Models
                </div>
            </div>
            <br>
            <img class="img-fluid" src="./Kangning Liu_files/kangning.jpeg" alt="Kangning Liu Profile Picture"
                 style="max-width:90%;height:auto; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);" loading="lazy">
                 
            <!-- Social Icons & CV -->
            <div style="margin-top: 25px; display: flex; justify-content: center; gap: 15px; flex-wrap: wrap;">
                <a href="https://scholar.google.com/citations?user=F3F2qAkAAAAJ&hl=en" target="_blank" style="font-size: 15px; color: var(--heading-color); transition: color 0.3s;"><i class="fas fa-graduation-cap"></i></a>
                <a href="https://github.com/Kangningthu" target="_blank" style="font-size: 15px; color: var(--heading-color); transition: color 0.3s;"><i class="fab fa-github"></i></a>
                <a href="https://www.linkedin.com/in/kangning-liu-14600282/" target="_blank" style="font-size: 15px; color: var(--heading-color); transition: color 0.3s;"><i class="fab fa-linkedin"></i></a>
                <a href="mailto:kangning.liu@nyu.edu" style="font-size: 15px; color: var(--heading-color); transition: color 0.3s;"><i class="fas fa-envelope"></i></a>
            </div>
        </div>


        <!-- Entries Column -->
        <div class="col-md-8 col-sm-12" style="padding-left: 30px;">

            <!-- Main Image -->
            <!--              -->
            <!--              <div style="font-family: &#39;Oswald&#39;, sans-serif; font-size: 32px;"><b>Kangning Liu (刘康宁)</b></div><br>-->
            <!--              <p><b>kangning.liu[at]nyu.edu</b><br>-->
            <!--            </p><p>Ph.D. student at <a href="https://cds.nyu.edu/">NYU Center for Data Science</a>.-->
            <!-- Introduction -->
            <p style="margin-top:3%; text-align:justify;">
                Hello! I am Kangning Liu, currently a Research Scientist at <a href="https://www.adobe.com/">Adobe</a>. 
                I earned my Ph.D. at the <a href="https://cds.nyu.edu/">NYU Center for Data Science</a>, where I was advised by <a
                    href="https://cims.nyu.edu/~cfgranda/">Prof. Carlos Fernandez Granda</a> and <a
                    href="https://cs.nyu.edu/~kgeras/">Prof. Krzysztof J. Geras</a>. Before that, I earned my <a
                    href="https://inf.ethz.ch/studies/master/master-ds.html">M.Sc. in Data Science from ETH Zurich</a>
                and my B.E. in Electrical Engineering from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua
                University</a>.
            </p>

            <p style="margin-top:3%; text-align:justify;">
                <strong>Research Focus:</strong> I build large-scale vision and multimodal foundation models that bridge cutting-edge research with real-world industrial impact. 
            </p>
            <p style="margin-top:3%; text-align:justify;">    
                My current work focuses on advancing multimodal understanding and integrating powerful visual comprehension into generative models. These frameworks enable a wide spectrum of applications, from GenAI data creation and reward modeling to equipping intelligent agents with complex tool-calling capabilities.
                My research has been published at top-tier venues (e.g., <strong>NeurIPS, CVPR, ICCV, ICLR</strong>) and deployed in production features across <strong>Adobe Photoshop</strong>, <strong>Lightroom</strong>, <strong>Express</strong>, and <strong>Firefly</strong>, impacting millions of users worldwide.
            </p>
 

            <!-- <p style="margin-top:3%; text-align:justify;">
                <strong>Research Focus:</strong> My research centers on multimodal large language models, vision foundation models, and large-scale data/annotation systems—bridging cutting-edge research with real-world impact.
                <!-- I have published at top-tier venues (e.g. <strong>NeurIPS, CVPR, ICCV</strong>) and successfully transferred technology into production systems across key Adobe products including <strong>Photoshop</strong>, <strong>Lightroom</strong>, <strong>Express</strong>, and <strong>Firefly</strong>. -->
            <!-- </p> -->
            <!-- <p style="margin-top:3%; text-align:justify;">    
                My current work focuses on fine-grained image understanding through vision-language modeling, enabling applications from generative AI data creation and reward modeling to AI agent tool calling and creative workflows. For example, the "selectionByPrompt" feature <a href="https://blog.adobe.com/en/publish/2025/12/10/edit-photoshop-chatgpt">[Adobe Blog]</a> enables users to select things in an image using natural language with <strong>Adobe Photoshop</strong> in <a href="https://chatgpt.com/">ChatGPT</a>.
                My previous research on segmentation foundation models has been deployed in the latest selection tools in <strong>Adobe Photoshop</strong> <a href="https://www.youtube.com/watch?v=ZJdmZ10I4H4">[Media Review]</a>, remove background in <strong>Adobe Firefly</strong> <a href="https://helpx.adobe.com/firefly/web/edit-images/transform-images-using-generative-fill/replace-background.html">[Feature Summary]</a>, and segmentation enhancements in <strong>Adobe Lightroom</strong> <a href="https://helpx.adobe.com/lightroom-classic/help/whats-new/2025-3.html">[Feature Summary]</a> and <strong>Adobe Express</strong>—enabling users to isolate complex image elements with remarkable speed and precision.
            </p> -->

            <!-- <p style="margin-top:3%; text-align:justify;">
                During my Ph.D., I contributed to a range of research projects focused on learning under imperfect supervision. These include uncertainty-aware fine-tuning of segmentation foundation models (<a href="https://kangning-liu.github.io/SUM_website/">SUM</a>), noise-resilient
                deep segmentation (<a href="https://github.com/Kangningthu/ADELE">ADELE</a>), weakly supervised segmentation (<a href="https://github.com/nyukat/GLAM">GLAM</a>), and unsupervised/self-supervised
                learning (<a href="https://arxiv.org/abs/2210.09452">ItS2CLR</a>). Beyond this, my expertise extends to
                video analysis (<a href="https://openreview.net/pdf?id=jIIzJaMbfw">StrokeRehab</a>) and video synthesis
                (<a href="https://arxiv.org/abs/2004.06502">UVIT</a> & <a href="https://arxiv.org/abs/2304.14471">Controllable
                Face Video Synthesis</a>).
            </p> -->

            <!-- News / Recent Updates Section -->
            <div style="margin-top: 3%; background-color: var(--card-bg); padding: 20px; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.05); border-left: 4px solid var(--nav-hover);">
                <h3 style="margin-top: 0; margin-bottom: 15px; font-size: 14px;">Recent Updates</h3>
                <ul style="margin-bottom: 0; padding-left: 20px;">
                    <li style="margin-bottom: 8px;"><b>Feb 2026:</b> Our paper "VGent" and "Sparse-LaViDa" were accepted to <strong>CVPR 2026</strong>!</li>
                    <li style="margin-bottom: 8px;"><b>Jan 2026:</b> Our paper "LaViDa-O" was accepted to <strong>ICLR 2026</strong>!</li>
                    <li style="margin-bottom: 8px;"><b>Dec 2025:</b> Released "selectionByPrompt" feature in <strong>Adobe Photoshop</strong> integrating into ChatGPT.</li>
                    <li><b>July 2025:</b> Our paper "RAS" was accepted to <strong>ICCV 2025</strong>!</li>
                </ul>
            </div>
            <!-- <p style="margin-top:3%; text-align:justify;">
            I am currently seeking research interns for the Summer of 2025. Please feel free to drop me an email if you are interested.
            </p> -->
            <!-- Internship and Job Search -->
<!--                I was a Research Intern at <a href="https://about.google">Google</a> in Mountain View during the summer-->
<!--                of 2022. During the summer of 2023, I was a Research Scientist Intern at <a-->
<!--                    href="https://www.adobe.com/">Adobe</a> in San Jose, working on advancing the segmentation-->
<!--                foundation model with more than 100 million images.-->

                <!-- Contact Info and Links -->
            <p style="margin-top:3%; text-align:justify;">
                For more details, feel free to reach out via email at <b>kangning.liu[at]nyu.edu</b> (still active). You can also find me on <a
                    href="https://scholar.google.com/citations?user=F3F2qAkAAAAJ&hl=en">Google Scholar <i
                    class="fas fa-graduation-cap"></i></a> and <a
                    href="https://www.linkedin.com/in/kangning-liu-14600282/">LinkedIn <i
                    class="fab fa-linkedin"></i></a>.
            </p>


        </div>
    </section>


    <section class="col-md-12">
        <h2 id="experience">Research Experience</h2>

        <ul class="vert">
            <li>
                <div class="company-header">
                    <img src="./Kangning Liu_files/Adobe.png" alt="Adobe Logo" loading="lazy">
                    <p class="company-title">
                        <strong>Adobe</strong>
                    </p>
                </div>
                <p class="location-time">San Jose, California, USA (April 2024 - present)</p>
                <p class="position-advisors">Research Scientist</p>

                <ul class="project-list alt justified-text">
    
                    <li><strong>Multimodal Grounding &amp; Reasoning:</strong> Enhance the grounding ability of multimodal large language models by integrating advanced reasoning capabilities and fine-grained understanding. These models enable a wide spectrum of applications, ranging from generative AI data creation and reward modeling for GenAI training to AI agent tool calling. For example, our "selectionByPrompt" feature <a href="https://blog.adobe.com/en/publish/2025/12/10/edit-photoshop-chatgpt">[Adobe Blog]</a> enables users to select things in an image using natural language with <strong>Adobe Photoshop</strong> in <a href="https://chatgpt.com/">ChatGPT</a>.
                        <br><span style="font-size: 0.9em; color: #7f8c8d; margin-right: 5px;">Related publications:</span> <a href="#pub-vgent" class="pub-link"><i class="fas fa-file-alt"></i> VGent [CVPR 2026]</a> <a href="#pub-ras" class="pub-link"><i class="fas fa-file-alt"></i> RAS [ICCV 2025]</a></li>
                    <li><strong>Unified Understanding &amp; Generation:</strong> Working on unified frameworks that bring powerful visual comprehension—reasoning, grounding, and fine-grained perception—into generative models, enabling a single architecture to both understand and create visual content.
                        <br><span style="font-size: 0.9em; color: #7f8c8d; margin-right: 5px;">Related publications:</span> <a href="#pub-lavida-o" class="pub-link"><i class="fas fa-file-alt"></i> LaViDa-O [ICLR 2026]</a>  <a href="#pub-sparse-lavida" class="pub-link"><i class="fas fa-file-alt"></i> Sparse-LaViDa [CVPR 2026]</a> <a href="#pub-lavida-r1" class="pub-link"><i class="fas fa-file-alt"></i> LaViDa-R1 [Preprint]</a> </li>
                    <li><strong>Vision Foundation Model:</strong> Developed cutting-edge segmentation foundation models to enhance Adobe products. 
                        Contributions include core features deployed in Adobe Photoshop's newest selection tools<a href="https://www.youtube.com/watch?v=ZJdmZ10I4H4"> [Media Review]</a>, remove background in Adobe Firefly <a href="https://helpx.adobe.com/firefly/web/edit-images/transform-images-using-generative-fill/replace-background.html">[Feature Summary]</a>, and segmentation enhancements in Adobe Lightroom <a href="https://helpx.adobe.com/lightroom-classic/help/whats-new/2025-3.html">[Feature Summary]</a> , Adobe Express.
                        <br><span style="font-size: 0.9em; color: #7f8c8d; margin-right: 5px;">Related publications:</span> <a href="#pub-sum" class="pub-link"><i class="fas fa-file-alt"></i> SUM [NeurIPS 2024]</a> <a href="#pub-sve" class="pub-link"><i class="fas fa-file-alt"></i> SVE [BMVC 2025]</a> <a href="#pub-consamme" class="pub-link"><i class="fas fa-file-alt"></i> conSAMmé [CVPRW 2025]</a></li>
                </ul>
            </li>
        </ul>


        <ul class="vert">
            <li>
                <div class="company-header">
                    <img src="./Kangning Liu_files/Adobe.png" alt="Adobe Logo" loading="lazy">
                    <p class="company-title">
                        <strong>Adobe</strong>
                    </p>
                </div>
                <p class="location-time">San Jose, California, USA (May 2023 - Nov 2023)</p>
                <p class="position-advisors">Research Intern | Advisors: Dr. Brian Price, Dr. Jason Kuen, Dr. Yifei Fan,
                    Dr. Zijun Wei, Luis Figueroa, Markus Woodson</p>
                <ul class="project-list alt justified-text">
                    <li><strong>Foundation Model Development & Scale:</strong> Worked on segmentation foundation model 
                        research, processing <strong>millions of images</strong> via a model-in-the-loop data engine, potentially influencing the roadmap for features in Adobe products such as Photoshop and Lightroom.
                    </li>
                    <li><strong>SOTA Performance & Quality Assurance:</strong> Achieved SOTA segmentation accuracy across 
                        20+ test sets (5-point IoU: <strong>0.869</strong>, surpassing SAM's 0.828). This work (SUM) was published 
                        in <strong>NeurIPS 2024</strong> [<a href="https://kangning-liu.github.io/SUM_website/">Project Website</a>] and established scalable feedback loops for iterative model 
                        refinement and product integration.
                    </li>
                </ul>
            </li>
        </ul>


        <!-- Google -->
        <ul class="vert">
            <li>
                <div class="company-header">
                    <img src="./Kangning Liu_files/Google.png" alt="Google Logo" loading="lazy">
                    <p class="company-title">
                        <strong>Google</strong>
                    </p>
                </div>
                <p class="location-time">Mountain View, California, USA (May 2022 - Sep 2022)</p>
                <p class="position-advisors">Research Intern | Advisors: Dr. Xuhui Jia, Dr. Yu-Chuan Su, Dr. Ruijin
                    Cang</p>
                <ul class="project-list alt justified-text">
                    <li><strong>One-shot Face Video Synthesis:</strong> Developed deep learning algorithms optimized for real-time one-shot face video synthesis, targeting efficient deployment on mobile devices.
                    </li>
                    <li><strong>Quality & Control:</strong> Innovated a method enhancing geometric correspondence (7% in AKD) and overall quality (15% in AED), while incorporating a plug-in interface for flexible emotion manipulation.
                    </li>
                </ul>
            </li>
        </ul>


        <br/>

        <ul class="vert">
            <li>
                <div class="company-header">
                    <img src="./Kangning Liu_files/NYU.png" alt="CDS Logo" loading="lazy">
                    <p class="company-title">
                        <strong>Center for Data Science,
                            New York University</strong>
                    </p>

                </div>
                <p class="location-time">New York, USA (Sept 2019 - March 2024)</p>
                <p class="position-advisors">Research Assistant | Advisors: Prof. Carlos Fernandez-Granda, Prof.
                    Krzysztof J. Geras</p>
                <ul class="project-list alt justified-text">
                    <li><strong>Robust Deep Learning with Noisy Labels:</strong> Engineered robust algorithms tailored for
                        segmentation tasks, showing resilience against annotation noise. This ensures the model's
                        reliability, especially in real-world scenarios where annotation noise is prevalent. This work (ADELE) 
                        was selected for <strong>CVPR 2022 Oral Presentation</strong> 
                        [<a href="https://github.com/Kangningthu/ADELE">GitHub</a>].
                    </li>
                    <li><strong>Weakly Supervised Learning:</strong> Proposed an innovative framework for multiple instance 
                        learning, which iteratively improves instance-level features by jointly estimating latent 
                        instance-level pseudo labels, and show that it outperforms existing methods on three real-world 
                        medical datasets. This work (ItS2CLR) was published in <strong>CVPR 2023</strong> 
                        [<a href="https://github.com/Kangningthu/ItS2CLR">GitHub</a>].
                    </li>
                    <li><strong>Video Understanding:</strong> Investigated and designed novel deep learning models for video 
                        action classification and segmentation, applied for the motion primitives prediction. This work (StrokeRehab) was published in 
                        <strong>NeurIPS 2022</strong> [<a href="https://openreview.net/forum?id=jIIzJaMbfw">Paper Link</a>].
                    </li>
                    <!-- <li><strong>Inverse Problem for Time Series Data:</strong> Engineered deep learning solutions for
                        MRI parameter estimation.
                    </li> -->
                </ul>
            </li>
        </ul>

        <br/>

        <!-- ETH Zurich -->
        <ul class="vert">
            <li>
                <div class="company-header">
                    <img src="./Kangning Liu_files/cvl2.png" alt="CVL Logo" loading="lazy">
                    <p class="company-title">
                        <strong>Computer Vision Laboratory, ETH Zurich</strong>
                    </p>
                </div>
                <p class="location-time">Zurich, Switzerland (Oct 2018 - Aug 2019)</p>
                <p class="position-advisors">Research Assistant | Advisors: Prof. Luc Van Gool, Prof. Radu Timofte,
                    Prof. Shuhang Gu</p>
                <ul class="project-list alt justified-text">
                    <li><strong>Domain Adaptation:</strong> Improved cross-domain semantic segmentation methods for
                        urban scene images through GAN models, harnessing weak paired information.
                    </li>
                    <li><strong>Video and Image Synthesis:</strong> Achieved state-of-the-art performance in
                        unsupervised multimodal video translation through bidirectional recurrent neural networks. 
                    </li>
                </ul>
            </li>
        </ul>
    </section>

    <!-- Links on the Sidebar -->

    <section class="col-md-12">
        <!-- Old: <div class="col-md-8" style="height: 120vh;"> -->
        <h2 id="publications">Publications</h2>

        <p>See also <a href="https://scholar.google.com/citations?user=F3F2qAkAAAAJ&hl=en">Google Scholar</a>.</p>

        <div class="filter-container" role="toolbar" aria-label="Filter publications by category">
            <button class="filter-btn active" data-filter="all">All</button>
            <button class="filter-btn" data-filter="mllm">MLLM</button>
            <button class="filter-btn" data-filter="foundation">Vision Foundation Model</button>
            <button class="filter-btn" data-filter="imperfect">Robust Learning</button>
            <button class="filter-btn" data-filter="video-gen">Video Generation</button>
            <button class="filter-btn" data-filter="video-understanding">Video Understanding</button>
            <button class="filter-btn" data-filter="others">Others</button>
            <div id="filter-count" style="margin-top: 10px; color: #7f8c8d; font-size: 12px; font-style: italic;"></div>
        </div>

        <ul class="vert pub-list">
            <li class="publication-item" id="pub-lavida-r1" data-category="mllm">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/Lavida-R1.png" alt="LaViDa-R1 framework for unified multimodal reasoning with diffusion language models"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models </b></p>
                        <p>
                            <a href="https://arxiv.org/abs/2602.14147" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p>Shufan Li, Yuchen Zhu, Jiuxiang Gu, <u><b>Kangning Liu</b></u>, Zhe Lin, Yongxin Chen, Molei Tao, Aditya Grover, Jason Kuen</p>
                        <p>[Preprint]</p>
                        <p class="justified-text">
                            Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. 
                            The latest works further extended them to multimodal understanding and generation tasks. 
                            In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. 
                            Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. 
                            In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary likelihood estimation, to enhance effectiveness and scalability. 
                            Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing.
                        </p>
                    </div>
                </div>
            </li>
            <li class="publication-item" id="pub-vgent" data-category="mllm">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/crop_vgent_method_v2.png" alt="VGent modular architecture for disentangling reasoning and prediction in visual grounding"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>VGent: Visual Grounding via Modular Design for Disentangling Reasoning and Prediction</b></p>
                        <p>
                            <a href="https://arxiv.org/pdf/2512.11099" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                            <a href="https://mp.weixin.qq.com/s/R_RVJPAqzgfvn65AJYlvnw" class="pub-link"><i class="fas fa-newspaper"></i> Media Coverage by 新智元</a>
                        </p>
                        <p>Weitai Kang, Jason Kuen, Mengwei Ren, Zijun Wei, Yan Yan, <u><b>Kangning Liu*</b></u>. (*project lead)</p>
                        <p>[CVPR 2026]</p>
                        <p class="justified-text">
                            Current visual grounding models either rely on auto-regressive MLLMs (slow with hallucination risks) or re-align LLMs with vision features (potentially undermining reasoning). 
                            In contrast, we propose <strong>VGent</strong>, a modular encoder-decoder architecture that explicitly disentangles high-level reasoning and low-level bounding box prediction. 
                            A frozen MLLM serves as the encoder for powerful reasoning, while a decoder selects target box(es) from detector-proposed candidates via cross-attention on encoder hidden states. 
                            This design leverages advances in both object detection and MLLMs, avoids auto-regressive pitfalls, and enables fast inference. 
                            We introduce: (i) <strong>QuadThinker</strong>, an RL-based training paradigm for multi-target reasoning; (ii) <strong>mask-aware labels</strong> for resolving detection-segmentation ambiguity; and (iii) <strong>global target recognition</strong> to improve target identification. 
                            VGent achieves state-of-the-art performance with <strong>+20.6% F1</strong> improvement, <strong>+8.2% gIoU</strong>, and <strong>+5.8% cIoU</strong> gains under visual reference challenges, while maintaining constant, fast inference.
                        </p>
                    </div>
                </div>
            </li>
            <li class="publication-item" id="pub-sparse-lavida" data-category="mllm">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/Sparse-LaVida.png" alt="Sparse-LaViDa framework for accelerating masked discrete diffusion model sampling"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Model </b></p>
                        <p>
                            <a href="https://arxiv.org/abs/2512.14008" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p>Shufan Li, Jiuxiang Gu, <u><b>Kangning Liu</b></u>, Zhe Lin, Zijun Wei, Aditya Grover, Jason Kuen.</p>
                        <p>[CVPR 2026]</p>
                        <p>
                            Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. 
                            In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. 
                            To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. 
                            Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. 
                        </p>
                    </div>
                </div>
            </li>
            <li class="publication-item" id="pub-lavida-o" data-category="mllm">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/LaViDa-O.png" alt="LaViDa-O elastic masked diffusion model for unified multimodal understanding and generation"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>LaViDa-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation </b></p>
                        <p>
                            <a href="https://arxiv.org/abs/2509.19244" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                            <a href="https://homepage.jackli.org/projects/lavida_o/index.html" class="pub-link"><i class="fas fa-globe"></i> Project Website</a>
                        </p>
                        <p>Shufan Li, Jiuxiang Gu, <u><b>Kangning Liu</b></u>, Zhe Lin, Zijun Wei, Aditya Grover, Jason Kuen.</p>
                        <p>[ICLR 2026]</p>
                        <p>
                            We propose <strong>LaViDa-O</strong>, a unified Masked Diffusion Model (MDM) that unifies multimodal understanding and high-resolution generation (1024px) within a single framework. 
                            By incorporating a novel <strong>Elastic Mixture-of-Transformers (Elastic-MoT)</strong> architecture, LaViDa-O couples a lightweight generation branch with a robust understanding branch, enabling efficient token compression and stratified sampling. 
                            The model further leverages planning and iterative self-reflection to boost generation quality. 
                            LaViDa-O achieves state-of-the-art performance across benchmarks for object grounding (RefCOCO), text-to-image generation (GenEval), and image editing (ImgEdit), outperforming leading autoregressive and continuous diffusion models while offering considerable inference speedups. 
                        </p>
                    </div>
                </div>
            </li>
            <li class="publication-item" id="pub-ras" data-category="mllm">
                <div class="row">
                    <!-- Publication Details -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/RAS.jpg" alt="RAS framework for referring to segmentation mask groups with vision-language prompts"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Refer to Any Segmentation Mask Group With Vision-Language Prompts </b></p>
                        <p>
                            <a href="https://arxiv.org/abs/2506.05342" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                            <a href="https://research.adobe.com/news/adobe-research-breakthroughs-at-iccv-2025/" class="pub-link"><i class="fas fa-video"></i> Featured Demo</a>
                        </p>
                        <p>Shengcao Cao, Zijun Wei, Jason Kuen, <u><b>Kangning Liu</b></u>, Lingzhi Zhang, Jiuxiang Gu, HyunJoon Jung, Liang-Yan Gui, Yu-Xiong Wang</p>
                        <p>[ICCV 2025]</p>
                        <p>
                            We introduce a novel task of omnimodal referring expression segmentation (ORES). In this task, a model produces a group of masks based on arbitrary prompts specified by text only or text plus reference visual entities. 
                            We propose a novel framework to "Refer to Any Segmentation Mask Group" (RAS), which augments segmentation models with complex multimodal interactions and comprehension via a mask-centric large multimodal model. 
                        </p>
                    </div>
                </div>
            </li>

            <li class="publication-item" id="pub-sve" data-category="foundation">
                <div class="row">
                    <!-- Publication Details -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/SVE.png" alt="SVE sparse vision encoder framework for efficient high-resolution inference"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Leveraging Sparsity for Efficient Inference of High-Resolution Vision Foundation Models. </b></p>
                        <p>
                            <a href="https://bmva-archive.org.uk/bmvc/2025/assets/papers/Paper_661/paper.pdf" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p>Xin Xu, Jason Kuen, Brian L. Price, <u><b>Kangning Liu</b></u>, Zijun Wei, Yu-Xiong Wang.</p>
                        <p>  [BMVC 2025]</p>
                        <p class="justified-text">
                            We introduce Sparse Vision Encoder (SVE), a post-training optimization framework that exploits naturally emerging sparsity in attention maps to accelerate inference of vision encoders. SVE selectively applies sparsity in key layers, performs sparsity distillation, and leverages cross-layer consistency to reuse sparsity structures. Experiments on DINOv2, CLIP, and SAM2 demonstrate up to 80% speedup for high-resolution encoding while preserving model performance
                        </p>
                    </div>
                </div>
            </li>
            <li class="publication-item" id="pub-consamme" data-category="foundation">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/conSAMme.jpg" alt="conSAMme contrastive learning framework for consistent SAM segmentations"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>ConSAMmé: Achieving Consistent Segmentations with SAM </b></p>
                        <p>
                            <a href="https://openaccess.thecvf.com/content/CVPR2025W/NTIRE/html/Myers-Dean_conSAMme_Achieving_Consistent_Segmentations_with_SAM_CVPRW_2025_paper.html" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p>Josh Myers-Dean, <u><b>Kangning Liu</b></u>, Brian Price, Yifei Fan, Danna Gurari. </p>
                        <p>  [CVPR 2025 NTIRE Workshop.]</p>
                        <p class="justified-text">
                            Multi-output interactive segmentation methods generate multiple binary masks when given user guidance, such as clicks.
                            However, it is unpredictable whether the order of the masks will match or whether those masks will be the same when given slightly different user guidance. 
                            To address these issues, we propose conSAMmé, a contrastive learning framework that conditions on explicit hierarchical semantics and leverages weakly supervised part segmentation data and a novel episodic click sampling strategy. 
                            Evaluation of conSAMmé’s performance, click robustness, and mask ordering show substantial improvements to baselines with less than 1% extra training data compared to the amount of data used for the baseline. 
                        </p>
                    </div>
                </div>
            </li>


            <li class="publication-item" id="pub-sum" data-category="foundation imperfect">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/SUM_framework.png" alt="SUM uncertainty-aware fine-tuning framework for segmentation foundation models"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Uncertainty-aware Fine-tuning of Segmentation Foundation Models </b></p>
                        <p>
                            <a href="https://openreview.net/pdf?id=qNXRXUC90b" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                            <a href="https://github.com/Kangningthu/SUM.git" class="pub-link"><i class="fab fa-github"></i> GitHub</a>
                            <a href="https://kangning-liu.github.io/SUM_website/" class="pub-link"><i class="fas fa-globe"></i> Project Website</a>
                        </p>
                        <p><u><b>Kangning Liu</b></u>, Brian Price, Jason Kuen, Yifei Fan, Zijun Wei, Luis Figueroa,
                            Krzysztof J. Geras, Carlos Fernandez-Granda. </p>
                        <p> [NeurIPS 2024]</p>
                        <p class="justified-text">
                            The Segment Anything Model (SAM) is a large-scale foundation model that has revolutionized
                            segmentation methodology. Despite its impressive generalization ability, the segmentation
                            accuracy of SAM on images with intricate structures is unsatisfactory in many cases. We
                            introduce the Segmentation with Uncertainty Model (SUM), which enhances the accuracy of
                            segmentation foundation models by incorporating an uncertainty-aware training loss and
                            prompt sampling based on the estimated uncertainty of pseudo-labels. We evaluated the
                            proposed SUM on a diverse test set consisting of 22 public benchmarks, where it achieves
                            state-of-the-art results. Notably, our method consistently surpasses SAM by 3-6 points in
                            mean IoU and 4-7 in mean boundary IoU across point-prompt interactive segmentation rounds.
                        </p>
                    </div>
                </div>
            </li>

            <li class="publication-item" data-category="video-gen">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/approach_v3.png" alt="Controllable one-shot face video synthesis approach with semantic aware prior"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Controllable One-Shot Face Video Synthesis With Semantic Aware Prior </b></p>
                        <p>
                            <a href="https://arxiv.org/abs/2304.14471" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p><u><b>Kangning Liu</b></u>, Yu-Chuan Su, Wei(Alex) Hong, Ruijin Cang, Xuhui Jia.</p>
                        <p>[Preprint]</p>
                        <p>
                            We propose a method that leverages rich face prior information to generate face videos with
                            improved semantic consistency and expression preservation.
                            Our model improves the baseline by 7% in average keypoint distance and outperforms the
                            baseline by 15% in average emotion embedding distance, while also providing a convenient
                            interface for highly controllable generation in terms of pose and expression.
                        </p>
                    </div>
                </div>
            </li>

            <li class="publication-item" data-category="imperfect">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/contrastive_learning_illustration.png" alt="ItS2CLR iterative self-paced supervised contrastive learning for multiple instance learning"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>

                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Multiple Instance Learning via Iterative Self-Paced Supervised Contrastive Learning</b></p>
                        <p>
                            <a href="https://arxiv.org/abs/2210.09452" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                            <a href="https://github.com/Kangningthu/ItS2CLR" class="pub-link"><i class="fab fa-github"></i> GitHub</a>
                        </p>
                        <p><u><b>Kangning Liu*</b></u>,Weicheng Zhu* <u>(*Equal contribution)</u>, Yiqiu Shen, Sheng
                            Liu, Narges Razavian, Krzysztof J. Geras, Carlos Fernandez-Granda. </p>
                        <p> [CVPR 2023]</p>
                        <p class="justified-text">In this paper, we introduce Iterative Self-Paced Supervised Contrastive Learning (Its2CLR), a
                            novel method for learning high-quality instance-level representations in Multiple Instance
                            Learning (MIL). Key features of our method: 1) self-paced learning to handle label noise and
                            uncertainty; 2) supervised contrastive learning to learn discriminative instance-level
                            embeddings; 3) iterative refinement of instance labels for robust and accurate
                            classification.</p>

                    </div>
                </div>
            </li>
            <li class="publication-item" data-category="imperfect">
                <div class="row">
                    <!-- Publication Details -->
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/ADELE.jpeg" alt="ADELE adaptive early-learning correction for segmentation from noisy annotations"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Adaptive early-learning correction for segmentation from noisy annotations </b></p>
                        <p>
                            <a href="https://arxiv.org/abs/2110.03740" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                            <a href="https://github.com/Kangningthu/ADELE" class="pub-link"><i class="fab fa-github"></i> GitHub</a>
                        </p>
                        <p>Sheng Liu*, <u><b>Kangning Liu*</b></u><u> (*Equal contribution, order decided by coin
                            flip.)</u>, Weicheng Zhu, Yiqiu Shen, Carlos Fernandez-Granda. </p>
                        <p> [CVPR 2022. <span style="color: #c0392b;">Oral 4.2% acceptance rate</span>]</p>
                        <p class="justified-text">Deep learning in the presence of noisy annotations has been studied
                            extensively in classification, but much less in segmentation tasks. In this project, we
                            study the learning dynamics of deep segmentation networks trained on inaccurately-annotated
                            data and propose a new method for semantic segmentation ADaptive Early-Learning corrEction
                            (ADELE).</p>
                    </div>
                </div>
            </li>
            <li class="publication-item" data-category="video-understanding">
                <div class="row">
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/Strokerehab.jpeg" alt="Publication Image for StrokeRehab"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                        <img src="./Kangning Liu_files/StrokeRehab2.jpeg" alt="Publication Image for StrokeRehab"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>StrokeRehab: A Benchmark Dataset for Sub-second Action Identification </b></p>
                        <p>
                            <a href="https://openreview.net/pdf?id=jIIzJaMbfw" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p>Aakash Kaku*, <u><b>Kangning Liu*</b></u> <u>(*Equal contribution)</u>, Avinash Parnandi*,
                            Haresh Rengaraj Rajamohan, Anita Venkatesan, Audre Wirtanen, Natasha Pandit, Kannan
                            Venkataramanan, Heidi Schambra, Carlos Fernandez-Granda.</p>
                        <p>[NeurIPS 2022]</p>
                        <p class="justified-text">We introduce a new benchmark dataset for the identification of subtle
                            and short-duration actions. We also propose a novel seq2seq approach, which outperforms the
                            existing methods on the new as well as standard benchmark datasets.</p>
                    </div>
                </div>
            </li>

            <li class="publication-item" data-category="others">
                <div class="row">
                    <!-- Image Placeholder -->
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/Equal.jpeg" alt="Publication Image for Neural Collapse"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>

                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Are All Losses Created Equal: A Neural Collapse Perspective </b></p>
                        <p>
                            <a href="https://openreview.net/pdf?id=8rfYWE3nyXl" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p>Jinxin Zhou, Chong You, Xiao Li, <u><b>Kangning Liu</b></u>, Sheng Liu, Qing Qu, Zhihui Zhu.
                        </p>
                        <p>[NeurIPS 2022]</p>
                        <p class="justified-text">A broad family of loss functions leads to neural collapse solutions
                            hence are equivalent on training set; moreover, they exhibit largely identical performance
                            on test data as well.</p>
                    </div>
                </div>
            </li>

            <!-- Start of the paper: Cramér-Rao bound-informed training of neural networks for quantitative MRI -->
            <li class="publication-item" data-category="others">
                <div class="row">
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/MRF.jpeg" alt="Publication Image for Cramér-Rao"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Cramér-Rao bound-informed training of neural networks for quantitative MRI </b></p>
                        <p>
                            <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.29206" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p>Xiaoxia Zhang*, Quentin Duchemin*, <u><b>Kangning Liu*</b></u> <u>(*Equal contribution)</u>,
                            Sebastian Flassbeck, Cem Gultekin, Carlos Fernandez-Granda, Jakob Asslander.</p>
                        <p>[Magnetic Resonance in Medicine 2021]</p>
                        <p class="justified-text">To address the parameter estimation problem in heterogeneous parameter
                            spaces, we propose a theoretically well-founded loss function based on the Cramér-Rao bound
                            (CRB), which provides a theoretical lower bound for the variance of an unbiased
                            estimator.</p>
                    </div>
                </div>
            </li>
            <!-- End of the paper -->
            <!-- Start of the paper: Weakly-supervised High-resolution Segmentation of Mammography Images -->
            <li class="publication-item" data-category="imperfect">
                <div class="row">
                    <div class="custom-col-img" style="padding: 10px;">
                        <!--<img src="./Kangning Liu_files/Inference_framework.png" alt="Publication Image for Weakly-supervised High-resolution Segmentation" class="img-fluid publication-img" style="width: 100%;" loading="lazy">-->
                        <img src="./Kangning Liu_files/Visualization_example.png"
                             alt="Publication Image for Weakly-supervised High-resolution Segmentation"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Weakly-supervised High-resolution Segmentation of Mammography Images for Breast Cancer Diagnosis </b></p>
                        <p>
                            <a href="https://arxiv.org/abs/2106.07049" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                            <a href="https://github.com/nyukat/GLAM" class="pub-link"><i class="fab fa-github"></i> GitHub</a>
                        </p>
                        <p><u><b>Kangning Liu</b></u>, Yiqiu Shen, Nan Wu, Jakub Piotr Chdowski, Carlos
                            Fernandez-Granda, and Krzysztof J. Geras</p>
                        <p>[Medical Imaging with Deep Learning 2021]</p>
                        <p class="justified-text">
                            In this study, we unveil a new neural network model for weakly-supervised, high-resolution
                            image segmentation.
                            Focused on breast cancer diagnosis via mammography, our method first identifies regions of
                            interest and then segments them in detail.
                            Validated on a substantial, clinically-relevant dataset, our approach significantly
                            outperforms existing methods, improving lesion localization performance by up to 39.6% and
                            20.0% for benign and malignant cases, respectively.
                        </p>
                    </div>
                </div>
            </li>
            <!-- End of the paper -->
            <!-- Start of the paper: Unsupervised Multimodal Video-to-Video Translation -->
            <li class="publication-item" data-category="video-gen">
                <div class="row">
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/UVIT_HR_multimodal.jpeg"
                             alt="Publication Image for Unsupervised Multimodal Video-to-Video Translation"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                        <!--<img src="./Kangning Liu_files/UVIT_overview.png" alt="Publication Image for Unsupervised Multimodal Video-to-Video Translation" class="img-fluid publication-img" style="width: 100%;" loading="lazy">-->
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>Unsupervised Multimodal Video-to-Video Translation via Self-Supervised Learning</b></p>
                        <p>
                            <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Liu_Unsupervised_Multimodal_Video-to-Video_Translation_via_Self-Supervised_Learning_WACV_2021_paper.pdf" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                            <a href="https://uvit.netlify.app" class="pub-link"><i class="fas fa-globe"></i> Project Website</a>
                        </p>
                        <p><u><b>Kangning Liu*</b></u>, Shuhang Gu*<u>(*Equal contribution)</u>, Andrs Romero, and Radu
                            Timofte</p>
                        <p>[WACV 2021]</p>
                        <p class="justified-text">
                            In this project, we introduce an unsupervised video-to-video translation model that
                            decouples style and content.
                            Leveraging specialized encoder-decoder and bidirectional RNNs, our model excels in
                            propagating inter-frame details.
                            This architecture enables style-consistent translations and offers a user-friendly interface
                            for cross-modality conversion.
                            We also implement a self-supervised training approach using a novel video interpolation loss
                            that captures sequence-based temporal information.
                        </p>
                    </div>
                </div>
            </li>
            <!-- End of the paper -->
            <!-- Start of the paper: An interpretable classifier for high-resolution breast cancer screening images -->
            <li class="publication-item" data-category="imperfect">
                <div class="row">
                    <div class="custom-col-img" style="padding: 10px;">
                        <img src="./Kangning Liu_files/gmic.jpeg"
                             alt="Publication Image for An interpretable classifier for high-resolution breast cancer screening images"
                             class="img-fluid publication-img" style="width: 100%;" loading="lazy">
                    </div>
                    <div class="custom-col-text" style="padding: 10px;">
                        <p><b>An interpretable classifier for high-resolution breast cancer screening images </b></p>
                        <p>
                            <a href="https://www.sciencedirect.com/science/article/pii/S1361841520302723" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        </p>
                        <p>Shen, Yiqiu, Nan Wu, Jason Phang, Jungkyu Park, <u><b>Kangning Liu</b></u>, Sudarshini Tyagi,
                            Laura Heacock et al.</p>
                        <p>[Medical image analysis, 2021]</p>
                        <p class="justified-text">
                            Medical images differ from natural images in significantly higher resolutions and smaller
                            regions of interest.
                            Because of these differences, neural network architectures that work well for natural images
                            might not be applicable to medical image analysis.
                            In this work, we propose a novel neural network model to address these unique properties of
                            medical images.
                        </p>
                    </div>
                </div>
            </li>
            <!-- End of the paper -->

        </ul>

    </section>

    <section class="col-md-12">
        <h2 id="teaching">Teaching</h2>
        <ul class="vert">
            <li>
                <div style="display: flex; align-items: flex-start; gap: 20px;">
                    <div style="font-size: 20px; color: var(--nav-hover); margin-top: 5px;"><i class="fas fa-chalkboard-teacher"></i></div>
                    <div>
                        <p style="margin-bottom: 5px; font-size: 11px; color: var(--heading-color);"><b>Probability and Statistics for Data Science</b></p>
                        <p style="margin-bottom: 5px; color: var(--muted-text);">NYU, Center for Data Science</p>
                        <p style="color: var(--heading-color);"><em>Teaching Assistant (Sept - Dec 2021)</em></p>
                    </div>
                </div>
            </li>
            <li>
                <div style="display: flex; align-items: flex-start; gap: 20px;">
                    <div style="font-size: 20px; color: var(--nav-hover); margin-top: 5px;"><i class="fas fa-chalkboard-teacher"></i></div>
                    <div>
                        <p style="margin-bottom: 5px; font-size: 11px; color: var(--heading-color);"><b>Advanced Machine Learning</b></p>
                        <p style="margin-bottom: 5px; color: var(--muted-text);">ETH Zurich, Department of Computer Science</p>
                        <p style="color: var(--heading-color);"><em>Teaching Assistant (Sept - Dec 2018)</em></p>
                    </div>
                </div>
            </li>
        </ul>
    </section>

    <section class="col-md-12">
        <h2 id="service">Service</h2>
        <ul class="vert">
            <li>
                <div style="display: flex; align-items: flex-start; gap: 20px;">
                    <div style="font-size: 20px; color: #f1c40f; margin-top: 5px;"><i class="fas fa-trophy"></i></div>
                    <div>
                        <p style="margin-bottom: 5px; font-size: 11px; color: var(--heading-color);"><b>Awards</b></p>
                        <ul class="project-list alt">
                            <li><a href="https://neurips.cc/Conferences/2024/ProgramCommittee#top-reviewers">Top Reviewer</a> for NeurIPS 2024</li>
                        </ul>
                    </div>
                </div>
            </li>
            <li>
                <div style="display: flex; align-items: flex-start; gap: 20px;">
                    <div style="font-size: 20px; color: #2ecc71; margin-top: 5px;"><i class="fas fa-clipboard-check"></i></div>
                    <div>
                        <p style="margin-bottom: 5px; font-size: 11px; color: var(--heading-color);"><b>Conference Reviewer</b></p>
                        <p style="color: var(--text-color); line-height: 1.6;">CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML, AISTATS, AAAI, WACV</p>
                    </div>
                </div>
            </li>
            <!--                <p>Conference Reviewer for CVPR (2022, 2023, 2024), ICCV (2023), ECCV (2024), NeurIPS (2022, 2023, 2024), ICLR-->
<!--                    (2025), AISTATS (2024, 2025), AAAI (2023, 2024, 2025), WACV (2021, 2022, 2023) and MIDL (2022,-->
<!--                    2023) </p>-->
            <li>
                <div style="display: flex; align-items: flex-start; gap: 20px;">
                    <div style="font-size: 20px; color: #e74c3c; margin-top: 5px;"><i class="fas fa-book-open"></i></div>
                    <div>
                        <p style="margin-bottom: 5px; font-size: 11px; color: var(--heading-color);"><b>Journal Reviewer</b></p>
                        <p style="color: var(--text-color); line-height: 1.6;">TPAMI, TMLR, TNNLS, CVIU</p>
                    </div>
                </div>
            </li>
        </ul>
    </section>

    <section class="col-md-12">
        <h2 id="photography">Miscellaneous</h2>

        <ul class="vert">
            <li>
                <p>Capturing moments and exploring light with my Sony A7M4.</p>
            </li>
        </ul>
        <br/>
        <p class="imgGallery-section-title">Visual Fragments</p>
        
        <div class="photo-filter-container" role="toolbar" aria-label="Filter photos by category">
            <button class="photo-filter-btn active" data-filter="all">All</button>
            <button class="photo-filter-btn" data-filter="california">California Style</button>
            <button class="photo-filter-btn" data-filter="nyc">NYC Moments</button>
            <button class="photo-filter-btn" data-filter="night">Night and Light</button>
            <button class="photo-filter-btn" data-filter="industrial">Industrial</button>
            <button class="photo-filter-btn" data-filter="nature">Nature & Travels</button>
        </div>

        <div class="imgGallery-row">
            <!-- California -->
            <div class="imgGallery-column" data-category="california">
                <a href="./Kangning Liu_files/DSC04444.jpeg" class="lightbox-gallery" title="California">
                    <img src="./Kangning Liu_files/DSC04444.jpeg" alt="California" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="california">
                <a href="./Kangning Liu_files/DSC04257.jpeg" class="lightbox-gallery" title="California">
                    <img src="./Kangning Liu_files/DSC04257.jpeg" alt="California" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="california">
                <a href="./Kangning Liu_files/DSC04158-2.jpeg" class="lightbox-gallery" title="California">
                    <img src="./Kangning Liu_files/DSC04158-2.jpeg" alt="California" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="california">
                <a href="./Kangning Liu_files/DSC06188.jpeg" class="lightbox-gallery" title="California">
                    <img src="./Kangning Liu_files/DSC06188.jpeg" alt="California" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="california">
                <a href="./Kangning Liu_files/DSC03609.jpeg" class="lightbox-gallery" title="California">
                    <img src="./Kangning Liu_files/DSC03609.jpeg" alt="California" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="california">
                <a href="./Kangning Liu_files/DSC07554-2.jpeg" class="lightbox-gallery" title="California">
                    <img src="./Kangning Liu_files/DSC07554-2.jpeg" alt="California" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="california">
                <a href="./Kangning Liu_files/DSC08122-2.jpeg" class="lightbox-gallery" title="California">
                    <img src="./Kangning Liu_files/DSC08122-2.jpeg" alt="California" loading="lazy">
                </a>
            </div>

            <!-- NYC -->
            <div class="imgGallery-column" data-category="nyc">
                <a href="./Kangning Liu_files/8V4B0383.jpg" class="lightbox-gallery" title="NYC">
                    <img src="./Kangning Liu_files/8V4B0383.jpg" alt="NYC" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nyc">
                <a href="./Kangning Liu_files/8V4B0444-2.jpg" class="lightbox-gallery" title="NYC">
                    <img src="./Kangning Liu_files/8V4B0444-2.jpg" alt="NYC" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nyc">
                <a href="./Kangning Liu_files/DSC04963-4.jpg" class="lightbox-gallery" title="NYC">
                    <img src="./Kangning Liu_files/DSC04963-4.jpg" alt="NYC" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nyc">
                <a href="./Kangning Liu_files/DSC05142.jpeg" class="lightbox-gallery" title="NYC">
                    <img src="./Kangning Liu_files/DSC05142.jpeg" alt="NYC" loading="lazy">
                </a>
            </div> 
            <div class="imgGallery-column" data-category="nyc">
                <a href="./Kangning Liu_files/DSC07398.jpg" class="lightbox-gallery" title="NYC">
                    <img src="./Kangning Liu_files/DSC07398.jpg" alt="NYC" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nyc">
                <a href="./Kangning Liu_files/DSC07301.jpg" class="lightbox-gallery" title="NYC">
                    <img src="./Kangning Liu_files/DSC07301.jpg" alt="NYC" loading="lazy">
                </a>
            </div> 
            <div class="imgGallery-column" data-category="nyc">
                <a href="./Kangning Liu_files/DSC08667.jpg" class="lightbox-gallery" title="NYC">
                    <img src="./Kangning Liu_files/DSC08667.jpg" alt="NYC" loading="lazy">
                </a>
            </div>

            <!-- Industrial & Night -->
            <div class="imgGallery-column" data-category="night">
                <a href="./Kangning Liu_files/DSC09256.jpeg" class="lightbox-gallery" title="Night">
                    <img src="./Kangning Liu_files/DSC09256.jpeg" alt="Night" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="night">
                <a href="./Kangning Liu_files/DSC09385.jpeg" class="lightbox-gallery" title="Night">
                    <img src="./Kangning Liu_files/DSC09385.jpeg" alt="Night" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="night">
                <a href="./Kangning Liu_files/DSC09265.jpeg" class="lightbox-gallery" title="Night">
                    <img src="./Kangning Liu_files/DSC09265.jpeg" alt="Night" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="industrial">
                <a href="./Kangning Liu_files/DSC06520-3.jpeg" class="lightbox-gallery" title="SteelRacks">
                    <img src="./Kangning Liu_files/DSC06520-3.jpeg" alt="SteelRacks" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="industrial">
                <a href="./Kangning Liu_files/DSC06601-2.jpeg" class="lightbox-gallery" title="SteelRacks">
                    <img src="./Kangning Liu_files/DSC06601-2.jpeg" alt="SteelRacks" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="industrial">
                <a href="./Kangning Liu_files/DSC06482-4.jpeg" class="lightbox-gallery" title="SteelRacks">
                    <img src="./Kangning Liu_files/DSC06482-4.jpeg" alt="SteelRacks" loading="lazy">
                </a>
            </div>

            <!-- Nature & Travels -->
            <div class="imgGallery-column" data-category="nature">
                <a href="./Kangning Liu_files/DSC05394.jpg" class="lightbox-gallery" title="Vermont">
                    <img src="./Kangning Liu_files/DSC05394.jpg" alt="Vermont" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nature">
                <a href="./Kangning Liu_files/DSC05554-2.jpg" class="lightbox-gallery" title="Vermont">
                    <img src="./Kangning Liu_files/DSC05554-2.jpg" alt="Vermont" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nature">
                <a href="./Kangning Liu_files/DSC05782.jpg" class="lightbox-gallery" title="Vermont">
                    <img src="./Kangning Liu_files/DSC05782.jpg" alt="Vermont" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nature">
                <a href="./Kangning Liu_files/DSC05962.jpg" class="lightbox-gallery" title="Oregon">
                    <img src="./Kangning Liu_files/DSC05962.jpg" alt="Oregon" loading="lazy">
                </a>
            </div>
            <div class="imgGallery-column" data-category="nature">
                <a href="./Kangning Liu_files/DSC05801.jpg" class="lightbox-gallery" title="Oregon">
                    <img src="./Kangning Liu_files/DSC05801.jpg" alt="Oregon" loading="lazy">
                </a>
            </div>
        </div>
    </section>

    <!-- /.container -->
    <!-- Other people may like it too! -->
    <!--    <a style="color:#b5bec9;font-size:0.8em; float:right;" href="https://github.com/mavroudisv/plain-academic">Plain Academic</a> -->

    <!-- <br/>

    <br/>
    <ul class="vert">
        <li>
            <p>Pageviews</p>
        </li>
    </ul> -->
    <div style="margin-top: 40px; padding: 20px; background-color: var(--card-bg); border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.05); text-align: center;">
        <p style="margin-bottom: 10px; color: var(--muted-text); font-size: 0.95rem;">Pageviews</p>
        <script type="text/javascript" id="clustrmaps"
                src="//clustrmaps.com/map_v2.js?d=nk2suWeO4QvLQb3OSDKbWMWpp_rc2lrGUiGq_rrnFQg&cl=ffffff&w=a"></script>
    </div>
</main>

<!-- Footer -->
<footer style="text-align: center; padding: 30px 20px; margin-top: 40px; border-top: 1px solid var(--border-color); color: var(--muted-text); font-family: 'Lato', sans-serif; font-size: 0.9rem;">
    <p>&copy; 2026 Kangning Liu. All rights reserved.</p>
</footer>

<!-- Back to Top Button -->
<button id="backToTop" title="Go to top"><i class="fas fa-arrow-up"></i></button>

<!-- Bootstrap 5 JS Bundle -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
<!-- jQuery (required for custom script) -->
<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/simplelightbox/2.14.2/simple-lightbox.min.js"></script>

    <script>
    $(document).ready(function() {
        // Button accessibility: mirror "active" visual state with aria-pressed
        function syncAriaPressed($buttons) {
            $buttons.each(function() {
                var isActive = $(this).hasClass('active');
                $(this).attr('aria-pressed', isActive ? 'true' : 'false');
            });
        }

        // Publication Filter Logic with Counter
        function updateFilterCount() {
            var visibleCount = $('.publication-item:visible').length;
            var totalCount = $('.publication-item').length;
            $('#filter-count').text(`Showing ${visibleCount} of ${totalCount} papers`);
        }

        $('.filter-btn').click(function() {
            var filterValue = $(this).attr('data-filter');
            
            // Update active button
            $('.filter-btn').removeClass('active');
            $(this).addClass('active');
            syncAriaPressed($('.filter-btn'));
            
            // Filter items
            if(filterValue === 'all') {
                $('.publication-item').fadeIn('fast').promise().done(function() {
                    updateFilterCount();
                });
            } else {
                $('.publication-item').each(function() {
                    var itemCategories = $(this).attr('data-category');
                    if(itemCategories && itemCategories.includes(filterValue)) {
                        $(this).fadeIn('fast');
                    } else {
                        $(this).fadeOut('fast');
                    }
                }).promise().done(function() {
                    updateFilterCount();
                });
            }
        });

        // Initialize Publication Counter
        updateFilterCount();
        syncAriaPressed($('.filter-btn'));

        // Handle in-page anchor clicks to publication items
        // Reset filter to "All" so the target is visible, then scroll to it
        $('a[href^="#pub-"]').on('click', function(e) {
            e.preventDefault();
            var targetId = $(this).attr('href');
            var $target = $(targetId);
            if ($target.length) {
                // Reset filter to "All"
                $('.filter-btn').removeClass('active');
                $('.filter-btn[data-filter="all"]').addClass('active');
                syncAriaPressed($('.filter-btn'));
                $('.publication-item').show();
                updateFilterCount();
                // Scroll to the target publication, then briefly highlight it
                $('html, body').animate({
                    scrollTop: $target.offset().top - 100
                }, 400, function() {
                    $target.css('transition', 'box-shadow 0.3s ease');
                    $target.css('box-shadow', '0 0 0 3px #3498db');
                    setTimeout(function() {
                        $target.css('box-shadow', '');
                        setTimeout(function() { $target.css('transition', ''); }, 300);
                    }, 1500);
                });
            }
        });

        // Photography Filter Logic
        $('.photo-filter-btn').click(function() {
            var filterValue = $(this).attr('data-filter');
            
            // Update active button
            $('.photo-filter-btn').removeClass('active');
            $(this).addClass('active');
            syncAriaPressed($('.photo-filter-btn'));
            
            // Filter items
            if(filterValue === 'all') {
                $('.imgGallery-column').fadeIn('fast');
            } else {
                $('.imgGallery-column').each(function() {
                    var itemCategory = $(this).attr('data-category');
                    if(itemCategory === filterValue) {
                        $(this).fadeIn('fast');
                    } else {
                        $(this).fadeOut('fast');
                    }
                });
            }
        });
        syncAriaPressed($('.photo-filter-btn'));

        // Lightbox Initialization
        var lightbox = new SimpleLightbox('.imgGallery-row a.lightbox-gallery', { 
            captionsData: 'alt',
            captionDelay: 250,
            animationSpeed: 250,
            fadeSpeed: 200
        });
        
        // Refresh Lightbox on filter change
        $('.photo-filter-btn').on('click', function() {
             setTimeout(function(){
                 lightbox.refresh();
             }, 300); 
        });

        // Back to Top Button
        var btn = $('#backToTop');

        $(window).scroll(function() {
            if ($(window).scrollTop() > 300) {
                btn.fadeIn();
            } else {
                btn.fadeOut();
            }
        });

        btn.on('click', function(e) {
            e.preventDefault();
            $('html, body').animate({scrollTop:0}, '300');
        });

        // Theme Toggle Logic
        const themeToggle = document.getElementById('theme-toggle');
        const themeIcon = document.getElementById('theme-icon');
        const root = document.documentElement;

        // Check for saved user preference or system preference
        const savedTheme = localStorage.getItem('theme');
        const systemPrefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        
        const initialTheme = savedTheme || (systemPrefersDark ? 'dark' : 'light');
        root.setAttribute('data-theme', initialTheme);
        updateIcon(initialTheme);

        themeToggle.addEventListener('click', () => {
            const currentTheme = root.getAttribute('data-theme');
            const newTheme = currentTheme === 'light' ? 'dark' : 'light';
            
            root.setAttribute('data-theme', newTheme);
            localStorage.setItem('theme', newTheme);
            updateIcon(newTheme);
        });

        function updateIcon(theme) {
            if (theme === 'dark') {
                themeIcon.classList.remove('fa-moon');
                themeIcon.classList.add('fa-sun');
            } else {
                themeIcon.classList.remove('fa-sun');
                themeIcon.classList.add('fa-moon');
            }
        }
    });
</script>

</body>
</html>

