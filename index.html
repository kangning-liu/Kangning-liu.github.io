<!DOCTYPE html>
<html lang="en"><head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-3GX2W3632C"></script>
    <script>window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-3GX2W3632C');</script>
    <!-- Google Tag Manager -->
    <script>(function (w, d, s, l, i) {
            w[l] = w[l] || []; w[l].push({
                'gtm.start':
                    new Date().getTime(), event: 'gtm.js'
            }); var f = d.getElementsByTagName(s)[0],
                j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
                    'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
        })(window, document, 'script', 'dataLayer', 'GTM-W6MLN9P');</script>
    <!-- End Google Tag Manager -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
<!--    <link rel="stylesheet" href="./Kangning Liu_files/cdnjs.cloudflare.com_ajax_libs_font-awesome_6.0.0-beta3_css_all.min.css">-->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">




    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <style>


        /* Reset */
body, h1, h2, h3, p {
    margin: 0;
    padding: 0;
}

/* Basic Styles */
.navbar {
    background-color: #333;
    color: white;
    font-family: Arial, sans-serif;
}

/* Style for navbar links */
.navbar-nav li {
    list-style-type: none;
}

.navbar-nav li a {
    color: white;
    padding: 15px;
    text-decoration: none;
    display: inline-block;
}

/* On Hover */
.navbar-nav li a:hover {
    background-color: #555;
    color: white;
}

/* Container */
.container {
    max-width: 1100px;
    margin: auto;
    padding: 0 20px;
}


        .imgGallery-row {
        display: flex;
        justify-content: center;
        align-items: center;
        }

        .imgGallery-column {
            flex: 33.33%;
            padding: 5px;
            box-sizing: border-box;
        }

        .imgGallery-column img {
            max-width: 100%;
            height: auto;
            /*border-radius: 8px;*/
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            transition: transform .2s ease-in-out;
        }

        .imgGallery-column img:hover {
            transform: scale(1.05);
        }

        .imgGallery-section-title {
            font-size: 14px;
            font-weight: bold;
            text-align: left;
            /*margin-bottom: 20px;*/
        }

        .company-header {
            display: grid;
            grid-template-columns: auto 1fr; /* Logo takes the space it needs, title takes the rest */
            align-items: center; /* Vertical centering */
            gap: 15px; /* Space between the logo and the name */
        }

        .company-header img {
                width: 60px;
                height: auto;
            }

        .company-title {
            margin: 0; /* Resetting any default margin */
        }

        .collapsible {
            cursor: pointer;
            text-decoration: underline;
        }

        .content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.2s ease-out;
        }

        .company-title {
            font-family: "Times New Roman", serif;
            font-size: 1.2em;
        }

        .location-time, .position-advisors {
            font-family: "Times New Roman", serif;
        }

        .project-list {
            font-family: "Times New Roman", serif;
            margin-top: 3%;
            text-align: justify;
        }
        /*        .publication-img {
            background-color: white;
        }*/

        /*.publication-image {*/
        /* Add white padding */
        /*padding: 20px;
            background-color: white;*/
        /* Rounded corners */
        /*border-radius: 10px;*/
        /* Box Shadow */
        /*box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);*/
        /* Align with text */
        /*float: left;*/ /* or you can use 'right' */
        /*margin-right: 20px;*/ /* Adjust as needed */
        /*}*/


        .publication-img {
            border: 10px solid white; /* This adds 'padding' */
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            margin-right: 20px; /* Space between image and text */
            background-color: white !important;
        }

        .row {
            display: flex;
            flex-wrap: wrap; /* updated from nowrap to wrap */
        }
        .justified-text {
          text-align: justify;
        }

        /* Default (Mobile First) */
        .custom-col-img,
        .custom-col-text {
            flex: 0 0 100%;
            max-width: 100%;
            /*margin-top: 3%;*/
            /*text-align: justify;*/
        }

        /* Landscape */
        @media screen and (orientation: landscape) {
            .custom-col-img {
                flex: 0 0 38.2%;
                max-width: 38.2%;
            }

            .custom-col-text {
                flex: 0 0 61.8%;
                max-width: 61.8%;
                margin-top: 3%;
                text-align: justify;
            }
        }

        /* Portrait */
        @media screen and (orientation: portrait) {
            .custom-col-img {
                flex: 0 0 100%;
                max-width: 100%;
            }

            .custom-col-text {
                flex: 0 0 100%;
                max-width: 100%;
                margin-top: 3%;
                text-align: justify;
            }
        }


        .publication-container {
            position: relative;
        }

        .publication-image {
            float: left;
            margin-right: 20px;
        }

        .publication-details {
        }

        .publication-image img {
            width: 100%;
            height: auto;
        }

        .clearfix {
            clear: both;
        }

        a.buttan:link, a.buttan:visited {
            color: black;
            padding: 0px 3px;
            margin: 1px 0px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            border: 1px solid black;
            position: relative;
        }

        a.nf:link, a.nf:visited, .nf {
            background-color: #b1d2ac;
        }

        a.nf:hover, a.nf:active {
            background-color: #b1d2ac;
        }

        a.free:link, a.free:visited, .free {
            background-color: #89C5EA;
        }

        a.free:hover, a.free:active, {
            background-color: #89C5EA;
        }

        a.talk:link, a.talk:visited, .talk {
            background-color: #89C5EA;
        }

        a.talk:hover, a.talk:active, {
            background-color: #89C5EA;
        }

        a.aux:link, a.aux:visited, .aux {
            background-color: #A08E7A;
            color: white;
        }

        a.aux:hover, a.aux:active {
            background-color: #A08E7A;
        }

        ul.vert {
            border-left: 0px solid red;
            background-color: #f1f1f1;
            list-style-type: none;
        }

            ul.vert > li {
                border-left: 10px solid #d1d1d1;
                background-color: #f1f1f1;
                list-style-type: none;
                padding: 10px 20px;
                margin-bottom: 10px;
                margin-left: -45px;
            }

        a.buttan:after {
            padding: 5px;
            display: none;
            position: absolute;
            text-align: center;
            color: #111111;
            top: -30px;
            right: 30px;
            width: 200px;
            background-color: #fef4c5;
            border: 1px solid #d4b943;
            -moz-border-radius: 2px;
            -webkit-border-radius: 2px;
            -ms-border-radius: 2px;
            border-radius: 2px;
            z-index: 98;
        }

        a.buttan:hover:after {
            display: block;
        }

        h2 {
            font-weight: 100;
        }

        /* LAYOUT CSS */
        .pure-img-responsive {
            max-width: 100%;
            height: auto;
        }

        #layout {
            padding: 0;
        }

        .header {
            text-align: center;
            top: auto;
            margin: 3em auto;
        }

        .sidebar {
            background: rgb(61, 79, 93);
            color: #fff;
        }

        .brand-title,
        .brand-tagline {
            margin: 0;
        }

        .brand-tagline {
            font-weight: 300;
            color: rgb(176, 202, 219);
        }

        .nav-list {
            margin: 0;
            padding: 0;
            list-style: none;
        }

        .nav-item {
            display: inline-block;
            *display: inline;
            zoom: 1;
        }

            .nav-item a {
                background: transparent;
                border: 2px solid rgb(176, 202, 219);
                color: #fff;
                margin-top: 1em;
                letter-spacing: 0.05em;
                text-transform: uppercase;
                font-size: 85%;
            }

                .nav-item a:hover,
                .nav-item a:focus {
                    border: 2px solid rgb(61, 146, 201);
                    text-decoration: none;
                }

        .content-subhead {
            text-transform: uppercase;
            color: #aaa;
            border-bottom: 1px solid #eee;
            padding: 0.4em 0;
            font-size: 80%;
            font-weight: 500;
            letter-spacing: 0.1em;
        }

        .content {
            padding: 2em 1em 0;
        }

        .post {
            padding-bottom: 2em;
        }

        .h2 {
            font-size: 2em;
            color: #222;
            margin-bottom: 0.2em;
        }

        p {
            font-family: Georgia, "Cambria", serif;
            color: #444;
            line-height: 1.8em;
        }

        .post-meta {
            color: #999;
            font-size: 90%;
            margin: 0;
        }

        .post-category {
            margin: 0 0.1em;
            padding: 0.3em 1em;
            color: #fff;
            background: #999;
            font-size: 80%;
        }

        .post-category-design {
            background: #5aba59;
        }

        .post-category-pure {
            background: #4d85d1;
        }

        .post-category-yui {
            background: #8156a7;
        }

        .post-category-js {
            background: #df2d4f;
        }

        .post-images {
            margin: 1em 0;
        }

        .post-image-meta {
            margin-top: -3.5em;
            margin-left: 1em;
            color: #fff;
            text-shadow: 0 1px 1px #333;
        }

        .footer {
            text-align: center;
            padding: 1em 0;
        }

            .footer a {
                color: #ccc;
                font-size: 80%;
            }

            .footer .pure-menu a:hover,
            .footer .pure-menu a:focus {
                background: none;
            }

        @media (min-width: 48em) {
            .content {
                padding: 2em 3em 0;
                margin-left: 25%;
            }

            .header {
                margin: 80% 2em 0;
                text-align: right;
            }

            .sidebar {
                position: fixed;
                top: 0;
                bottom: 0;
            }

            .nav-item {
                display: block;
            }
        }


    </style>

    <link rel="stylesheet" href="./Kangning Liu_files/pure-min.css">
    <link rel="stylesheet" href="./Kangning Liu_files/grids-responsive-min.css">


    <!-- Load Bootstrap CSS securely -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">

    <!-- Load your custom CSS -->
    <link rel="stylesheet" href="firstcss.css">

    <!-- Load the font from Google Fonts -->
    <link href='https://fonts.googleapis.com/css?family=Oswald:700' rel='stylesheet' type='text/css'>

    <!-- Load jQuery and Bootstrap JS securely -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

    <title>Kangning Liu</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--  <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">-->
    <link rel="stylesheet" href="./Kangning Liu_files/firstcss.css">
    <script src="./Kangning Liu_files/jquery.min.js"></script>
    <script src="./Kangning Liu_files/bootstrap.min.js"></script>
    <link href="./Kangning Liu_files/css" rel="stylesheet" type="text/css">
    <style>
        * {
            box-sizing: border-box;
        }

        .column {
            float: left;
            width: 33.33%;
            padding: 5px;
        }

        /* Clearfix (clear floats) */
        .row::after {
            content: "";
            clear: both;
            display: table;
        }
    </style>
</head>
<body data-new-gr-c-s-check-loaded="14.1088.0" data-gr-ext-installed="">
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W6MLN9P"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->

    <!-- Navigation -->
<nav class="navbar navbar-inverse">
    <div class="container" id="home">
        <ul class="nav navbar-nav">
          <li><a href="#home">Home</a></li>
          <li><a href="#experience">Research Experience</a></li>
          <li><a href="#publications">Publications</a></li>
          <li><a href="#teaching">Teaching</a></li>
          <li><a href="#service">Service</a></li>
          <li><a href="#photography">Miscellaneous</a></li>
        </ul>
    </div>
</nav>




  
  <!-- Page Content -->
    <div class="container">

        <div class="row">
            <div class="col-md-4 col-sm-12 custom-col-img">
                <!--<div style="font-size: 2rem;"><b>Kangning Liu (刘康宁)</b></div><br>-->
                <div style="font-size: 3rem; color: #000000; font-family: 'Georgia', serif;">
                    <b>Kangning Liu (刘康宁)</b>
                </div><br>
                <img class="img-responsive" src="./Kangning Liu_files/kangning.jpeg" alt="" style="max-width:60%;height:auto;">
            </div>


            <!-- Entries Column -->
            <div class="col-md-8 col-sm-12 custom-col-text">

                <!-- Main Image -->
                <!--              -->
                <!--              <div style="font-family: &#39;Oswald&#39;, sans-serif; font-size: 32px;"><b>Kangning Liu (刘康宁)</b></div><br>-->
                <!--              <p><b>kangning.liu[at]nyu.edu</b><br>-->
                <!--            </p><p>Ph.D. student at <a href="https://cds.nyu.edu/">NYU Center for Data Science</a>.-->
                <!-- Introduction -->
                <p style="margin-top:3%; text-align:justify;">
                    Hello! I am Kangning Liu, a Research Scientist at <a href=https://www.adobe.com/">Adobe</a>. I obtained my Ph.D. at the <a href="https://cds.nyu.edu/">NYU Center for Data Science</a>, where I am advised by <a href="https://cims.nyu.edu/~cfgranda/">Prof. Carlos Fernandez Granda</a> and <a href="https://cs.nyu.edu/~kgeras/">Prof. Krzysztof J. Geras</a>. Before that, I earned my <a href="https://inf.ethz.ch/studies/master/master-ds.html">M.Sc. in Data Science from ETH Zurich</a> and my B.E. in Electrical Engineering from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>.
                </p>
                <p style="margin-top:3%; text-align:justify;">
                    I am deeply passionate about harnessing the power of machine learning and computer vision to address tangible, real-world challenges. My research particularly centers on learning under imperfect supervision, such as uncertainty-aware fine-tuning of segmentation foundation models, noise-resilient deep segmentation (<a href="https://github.com/Kangningthu/ADELE">ADELE</a>), weakly supervised object localization (<a href="https://github.com/nyukat/GLAM">GLAM</a>), and unsupervised/self-supervised learning (<a href="https://arxiv.org/abs/2210.09452">ItS2CLR</a>). Beyond this, my expertise extends to video analysis (<a href="https://openreview.net/pdf?id=jIIzJaMbfw">StrokeRehab</a>) and video synthesis (<a href="https://arxiv.org/abs/2004.06502">UVIT</a> & <a href="https://arxiv.org/abs/2304.14471">Controllable Face Video Synthesis</a>).
                </p>

                <!-- Internship and Job Search -->
                <p style="margin-top:3%; text-align:justify;">
                    I was a Research Intern at <a href="https://about.google">Google</a> in Mountain View during the summer of 2022. During the summer of 2023, I was a Research Scientist Intern at <a href="https://www.adobe.com/">Adobe</a> in San Jose, working on advancing the segmentation foundation model with more than 100 million images.

                <!-- Contact Info and Links -->
                <p style="margin-top:3%; text-align:justify;">
                    For more details, feel free to contact me at <b>kangning.liu[at]nyu.edu</b>. You can also find me on <a href="https://scholar.google.com/citations?user=F3F2qAkAAAAJ&hl=en">Google Scholar <i class="fas fa-graduation-cap"></i></a> and <a href="https://www.linkedin.com/in/kangning-liu-14600282/">LinkedIn <i class="fab fa-linkedin"></i></a>.
                </p>


            </div>



            <div class="col-md-4" style="margin-top:1%">
                <div class="media">
                    <div>
                        <!--			<h3>News</h3>
                    <div class="text-justify">
                    -Watch this space.</p> -->
                    </div>
                </div>
            </div>
        </div>


        <div class="col-md-13">
            <h2 id="experience">Research Experience</h2>

            <ul class="vert">
                <li>
                    <div class="company-header">
                        <img src="./Kangning Liu_files/Adobe.png" alt="Adobe Logo">
                        <p class="company-title">
                            <strong>Adobe</strong>
                        </p>
                    </div>
                    <p class="location-time">San Jose, California, USA (April 2024 - present)</p>
                    <p class="position-advisors">Research Scientist</p>
                </li>
            </ul>



            <ul class="vert">
                <li>
                    <div class="company-header">
                        <img src="./Kangning Liu_files/Adobe.png" alt="Adobe Logo">
                        <p class="company-title">
                            <strong>Adobe</strong>
                        </p>
                    </div>
                    <p class="location-time">San Jose, California, USA (May 2023 - Nov 2023)</p>
                    <p class="position-advisors">Research Intern | Advisors: Dr. Brian Price, Dr. Jason Kuen, Dr. Yifei Fan, Dr. Zijun Wei, Luis Figueroa, Markus Woodson</p>
                    <ul class="project-list alt justified-text">
                        <li><strong>Segmentation Foundation Model:</strong> Engaged in a large-scale segmentation initiative that utilizes an expansive dataset of over 100 million diverse images. The work involved a model-in-the-loop data engine and close collaboration with teams from Adobe Research, Photoshop, and Lightroom</li>
                        <li><strong>SAM Model Reproducing and Beyond:</strong> Successfully reproduced the large-scale multi-node training and interactive segmentation capabilities of the <a href="https://segment-anything.com">SAM (Segment Anything Model)</a>, and further enhanced its features for accuracy and efficiency, potentially influencing the roadmap for future products.</li>
                        <li><strong>Human-Centric Feedback Loop: </strong> Initiated a human feedback mechanism by designing meticulous evaluation experiments and coordinating with the labeling team. This ensures that the models are attuned to nuanced user requirements and can be iteratively improved based on real-world feedback</li>
                        <li><strong>Benchmarking and Quality Assurance:</strong> Elevated model performance through algorithmic advancements, pushing the boundaries of model performance, achieving state-of-the-art segmentation accuracy across 20+ diverse test sets. This underscores the model’s potential for real-world applications.</li>
                    </ul>
                </li>
            </ul>



            <!-- Google -->
            <ul class="vert">
                <li>
                    <div class="company-header">
                        <img src="./Kangning Liu_files/Google.png" alt="Google Logo">
                        <p class="company-title">
                            <strong>Google</strong>
                        </p>
                    </div>
                    <p class="location-time">Mountain View, California, USA (May 2022 - Sep 2022)</p>
                    <p class="position-advisors">Research Intern | Advisors: Dr. Xuhui Jia, Dr. Yu-Chuan Su, Dr. Ruijin Cang</p>
                    <ul class="project-list alt justified-text">
                        <li><strong>One-shot Face Video Synthesis:</strong> Developed deep learning algorithms optimized for real-time face video synthesis, targeting efficient deployment on mobile devices such as smartphones.</li>
                        <li><strong>Improved Quality:</strong> Innovated a method that enhances both the geometric correspondence (7% in AKD) and overall quality (%15 in AED) of the face videos produced.</li>
                        <li><strong>Emotion Manipulation:</strong>  Incorporated a convenient interface to manipulate the expression and emotion of the generated video in a plug-in manner. </li>
                    </ul>
                </li>
            </ul>


            <br />

            <ul class="vert">
                <li>
                    <div class="company-header">
                        <img src="./Kangning Liu_files/NYU.png" alt="CDS Logo">
                        <p class="company-title">
                            <strong>Center for Data Science,
                                New York University</strong>
                        </p>

                        </div>
                    <p class="location-time">New York, USA (Sept 2019 - present)</p>
                    <p class="position-advisors">Research Assistant | Advisors: Prof. Carlos Fernandez-Granda, Prof. Krzysztof J. Geras</p>
                    <ul class="project-list alt justified-text">
                        <li><strong>Weakly Supervised High-Resolution Segmentation:</strong> Pioneered vision-based methods for weakly supervised object localization in the domain of breast cancer diagnosis.</li>
                        <li><strong>Noise-Resilient Deep Segmentation:</strong> Engineered robust algorithms tailored for segmentation tasks, showing resilience against annotation noise. This ensures the model’s reliability, especially in real-world scenarios where annotations noise is prevalent.</li>
                        <li><strong>Video Understanding:</strong> Created deep learning models aimed at video action classification and segmentation, applicable in predicting motion primitives.</li>
                        <li><strong>Unsupervised Model Finetuning via Weak Supervision:</strong> Devised an innovative multiple-instance learning framework that iteratively refines instance-level features and outperforms existing methods across three real-world datasets.</li>
                        <li><strong>Inverse Problem for Time Series Data:</strong> Engineered deep learning solutions for MRI parameter estimation.</li>
                    </ul>
                </li>
            </ul>

            <br />

            <!-- ETH Zurich -->
            <ul class="vert">
                <li>
                    <div class="company-header">
                        <img src="./Kangning Liu_files/cvl2.png" alt="CVL Logo">
                        <p class="company-title">
                            <strong>Computer Vision Laboratory, ETH Zurich</strong>
                        </p>
                    </div>
                    <p class="location-time">Zurich, Switzerland (Oct 2018 - Aug 2019)</p>
                    <p class="position-advisors">Research Assistant | Advisors: Prof. Luc Van Gool, Prof. Radu Timofte, Prof. Shuhang Gu</p>
                    <ul class="project-list alt justified-text">
                        <li><strong>Domain Adaptation:</strong> Improved cross-domain semantic segmentation methods for urban scene images through GAN models, harnessing weak paired information.</li>
                        <li><strong>Video and Image Synthesis:</strong> Achieved state-of-the-art performance in unsupervised multimodal video translation through bidirectional recurrent neural networks.</li>
                    </ul>
                </li>
            </ul>
        </div>

        <!-- Links on the Sidebar -->

        <div class="col-md-13">
            <!-- Old: <div class="col-md-8" style="height: 120vh;"> -->
            <h2 id="publications">Publications</h2>

            <p>See also <a href="https://scholar.google.com/citations?user=F3F2qAkAAAAJ&hl=en">Google Scholar</a>.</p>

            <h3>Preprint</h3>
            <ul class="vert">
                <li class="publication-item">
                    <div class="row">
                        <!-- Publication Details -->
                        <!-- Image Placeholder -->
                        <div class="custom-col-img" style="padding: 10px;">
                            <img src="./Kangning Liu_files/approach_v3.png" alt="Publication Image" class="img-fluid publication-img" style="width: 100%;">
                        </div>
                        <div class="custom-col-text" style="padding: 10px;">
                            <p><b>Controllable One-Shot Face Video Synthesis With Semantic Aware Prior </b></p>
                            <p>
                                <b>
                                    [<a href="https://arxiv.org/abs/2304.14471">Paper link</a>]
                                </b>
                            </p>
                            <p><u><b>Kangning Liu</b></u>, Yu-Chuan Su, Wei(Alex) Hong, Ruijin Cang, Xuhui Jia.</p>
                            <p>
                                We propose a method that leverages rich face prior information to generate face videos with improved semantic consistency and expression preservation.
                                Our model improves the baseline by 7% in average keypoint distance and outperforms the baseline by 15% in average emotion embedding distance, while also providing a convenient interface for highly controllable generation in terms of pose and expression.
                            </p>
                        </div>
                    </div>
                </li>
            </ul>


            <h3>Published</h3>
            <ul class="vert">
                <li class="publication-item">
                    <div class="row">
                        <!-- Publication Details -->
                        <!-- Image Placeholder -->
                        <div class="custom-col-img" style="padding: 10px;">
                            <img src="./Kangning Liu_files/sum_figure1.jpg" alt="Publication Image" class="img-fluid publication-img" style="width: 100%;">
                        </div>
                        <div class="custom-col-text" style="padding: 10px;">
                            <p><b>Uncertainty-aware Fine-tuning of Segmentation Foundation Models </b></p>
                            <p>
                                <b>
                                    [<a>Arxiv version coming soon</a>]
                                </b>
                            </p>
                            <p><u><b>Kangning Liu</b></u>, Brian Price, Jason Kuen, Yifei Fan, Zijun Wei, Luis Figueroa, Krzysztof J. Geras, Carlos Fernandez-Granda. </p>
                            <p> [NeurIPS 2024]</p>
                            <p class="justified-text">
                                The Segment Anything Model (SAM) is a large-scale foundation model that has revolutionized segmentation methodology. Despite its impressive generalization ability, the segmentation accuracy of SAM on images with intricate structures is unsatisfactory in many cases. We introduce a novel Segmentation with Uncertainty Model (SUM) that combines high-quality annotated data with a large unlabeled dataset in order to improve segmentation quality of SAM without forgetting. We evaluated the proposed SUM on a diverse test set consisting of 14 public benchmarks, where it achieves state-of-the-art results. Notably, our mean single-point mIoU is 0.751, and the 5 point-prompt mIoU is 0.869, surpassing SAM's scores of 0.690 and 0.828, respectively.
                            </p>
                        </div>
                    </div>
                </li>


                <li class="publication-item">
                    <div class="row">
                        <!-- Publication Details -->
                        <!-- Image Placeholder -->
                        <div class="custom-col-img" style="padding: 10px;">
                            <img src="./Kangning Liu_files/contrastive_learning_illustration.png" alt="Publication Image" class="img-fluid publication-img" style="width: 100%;">
                        </div>

                        <div class="custom-col-text" style="padding: 10px;">
                            <p>
                                <b>
                                    Multiple Instance Learning via Iterative Self-Paced Supervised Contrastive
                                    Learning
                                </b>
                            </p>
                            <p>
                                <b>
                                    [<a href="https://arxiv.org/abs/2210.09452">Paper link</a>]
                                    [<a href="https://github.com/Kangningthu/ItS2CLR">Github Link</span></a>]
                                </b>
                            </p>
                            <p><u><b>Kangning Liu*</b></u>,Weicheng Zhu* <u>(*Equal contribution)</u>, Yiqiu Shen, Sheng Liu, Narges Razavian, Krzysztof J. Geras, Carlos Fernandez-Granda. </p>
                            <p> [CVPR 2023]</p>
                            <p class="justified-text">
                                 <p>In this paper, we introduce Iterative Self-Paced Supervised Contrastive Learning (Its2CLR), a novel method for learning high-quality instance-level representations in Multiple Instance Learning (MIL). Key features of our method: 1) self-paced learning to handle label noise and uncertainty; 2) supervised contrastive learning to learn discriminative instance-level embeddings; 3) iterative refinement of instance labels for robust and accurate classification.</p>

                        </div>
                    </div>
                </li>
                <li class="publication-item">
                    <div class="row">
                        <!-- Publication Details -->
                        <!-- Image Placeholder -->
                        <div class="custom-col-img" style="padding: 10px;">
                            <img src="./Kangning Liu_files/ADELE.jpeg" alt="Publication Image" class="img-fluid publication-img" style="width: 100%;">
                        </div>
                        <div class="custom-col-text" style="padding: 10px;">
                            <p><b>Adaptive early-learning correction for segmentation from noisy annotations </b></p>
                            <p>
                                <b>
                                    [<a href="https://arxiv.org/abs/2110.03740">Paper Link</a>][<a href="https://github.com/Kangningthu/ADELE">Github Link</a>]
                                </b>
                            </p>
                            <p>Sheng Liu*, <u><b>Kangning Liu*</b></u><u> (*Equal contribution, order decided by coin flip.)</u>, Weicheng Zhu, Yiqiu Shen, Carlos Fernandez-Granda. </p>
                            <p> [CVPR 2022. <font color="red">Oral 4.2% acceptance rate</font>]</p>
                            <p class="justified-text">Deep learning in the presence of noisy annotations has been studied extensively in classification, but much less in segmentation tasks. In this project, we study the learning dynamics of deep segmentation networks trained on inaccurately-annotated data and propose a new method for semantic segmentation ADaptive Early-Learning corrEction (ADELE).</p>
                        </div>
                    </div>
                </li>
                <li class="publication-item">
                    <div class="row">
                        <!-- Image Placeholder -->
                        <div class="custom-col-img" style="padding: 10px;">
                            <img src="./Kangning Liu_files/Strokerehab.jpeg" alt="Publication Image for StrokeRehab" class="img-fluid publication-img" style="width: 100%;">
                            <img src="./Kangning Liu_files/StrokeRehab2.jpeg" alt="Publication Image for StrokeRehab" class="img-fluid publication-img" style="width: 100%;">
                        </div>
                        <div class="custom-col-text" style="padding: 10px;">
                            <p><b>StrokeRehab: A Benchmark Dataset for Sub-second Action Identification </b></p>
                            <p>
                                <b>
                                    [<a href="https://openreview.net/pdf?id=jIIzJaMbfw">Paper Link</a>]
                                </b>
                            </p>
                            <p>Aakash Kaku*, <u><b>Kangning Liu*</b></u> <u>(*Equal contribution)</u>, Avinash Parnandi*, Haresh Rengaraj Rajamohan, Anita Venkatesan, Audre Wirtanen, Natasha Pandit, Kannan Venkataramanan, Heidi Schambra, Carlos Fernandez-Granda.</p>
                            <p>[NeurIPS 2022]</p>
                            <p class="justified-text">We introduce a new benchmark dataset for the identification of subtle and short-duration actions. We also propose a novel seq2seq approach, which outperforms the existing methods on the new as well as standard benchmark datasets.</p>
                        </div>
                    </div>
                </li>

                <li class="publication-item">
                    <div class="row">
                        <!-- Image Placeholder -->
                        <div class="custom-col-img" style="padding: 10px;">
                            <img src="./Kangning Liu_files/Equal.jpeg" alt="Publication Image for Neural Collapse" class="img-fluid publication-img" style="width: 100%;">
                        </div>

                        <div class="custom-col-text" style="padding: 10px;">
                            <p><b>Are All Losses Created Equal: A Neural Collapse Perspective </b></p>
                            <p>
                                <b>
                                    [<a href="https://openreview.net/pdf?id=8rfYWE3nyXl">Paper Link</a>]
                                </b>
                            </p>
                            <p>Jinxin Zhou, Chong You, Xiao Li, <u><b>Kangning Liu</b></u>, Sheng Liu, Qing Qu, Zhihui Zhu.</p>
                            <p>[NeurIPS 2022]</p>
                            <p class="justified-text">A broad family of loss functions leads to neural collapse solutions hence are equivalent on training set; moreover, they exhibit largely identical performance on test data as well.</p>
                        </div>
                    </div>
                </li>

                <!-- Start of the paper: Cramér-Rao bound-informed training of neural networks for quantitative MRI -->
                <li class="publication-item">
                    <div class="row">
                        <div class="custom-col-img" style="padding: 10px;">
                            <img src="./Kangning Liu_files/MRF.jpeg" alt="Publication Image for Cramér-Rao" class="img-fluid publication-img" style="width: 100%;">
                        </div>
                        <div class="custom-col-text" style="padding: 10px;">
                            <p><b>Cramér-Rao bound-informed training of neural networks for quantitative MRI </b></p>
                            <p>
                                <b>
                                    [<a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.29206">Paper Link</a>]
                                </b>
                            </p>
                            <p>Xiaoxia Zhang*, Quentin Duchemin*, <u><b>Kangning Liu*</b></u>  <u>(*Equal contribution)</u>, Sebastian Flassbeck, Cem Gultekin, Carlos Fernandez-Granda, Jakob Asslander.</p>
                            <p>[Magnetic Resonance in Medicine 2021]</p>
                            <p class="justified-text">To address the parameter estimation problem in heterogeneous parameter spaces, we propose a theoretically well-founded loss function based on the Cramér-Rao bound (CRB), which provides a theoretical lower bound for the variance of an unbiased estimator.</p>
                        </div>
                    </div>
                </li>
                <!-- End of the paper -->
                <!-- Start of the paper: Weakly-supervised High-resolution Segmentation of Mammography Images -->
                <li class="publication-item">
                    <div class="row">
                        <div class="custom-col-img" style="padding: 10px;">
                            <!--<img src="./Kangning Liu_files/Inference_framework.png" alt="Publication Image for Weakly-supervised High-resolution Segmentation" class="img-fluid publication-img" style="width: 100%;">-->
                            <img src="./Kangning Liu_files/Visualization_example.png" alt="Publication Image for Weakly-supervised High-resolution Segmentation" class="img-fluid publication-img" style="width: 100%;">
                        </div>
                        <div class="custom-col-text" style="padding: 10px;">
                            <p><b>Weakly-supervised High-resolution Segmentation of Mammography Images for Breast Cancer Diagnosis </b></p>
                            <p>
                                <b>
                                    [<a href="https://arxiv.org/abs/2106.07049">Paper Link</a>][<a href="https://github.com/nyukat/GLAM">Github Link</a>]
                                </b>
                            </p>
                            <p><u><b>Kangning Liu</b></u>, Yiqiu Shen, Nan Wu, Jakub Piotr Chdowski, Carlos Fernandez-Granda, and Krzysztof J. Geras</p>
                            <p>[Medical Imaging with Deep Learning 2021]</p>
                            <p class="justified-text">
                                In this study, we unveil a new neural network model for weakly-supervised, high-resolution image segmentation.
                                Focused on breast cancer diagnosis via mammography, our method first identifies regions of interest and then segments them in detail.
                                Validated on a substantial, clinically-relevant dataset, our approach significantly outperforms existing methods, improving lesion localization performance by up to 39.6% and 20.0% for benign and malignant cases, respectively.
                            </p>
                        </div>
                    </div>
                </li>
                <!-- End of the paper -->
                <!-- Start of the paper: Unsupervised Multimodal Video-to-Video Translation -->
                <li class="publication-item">
                    <div class="row">
                        <div class="custom-col-img" style="padding: 10px;">
                            <img src="./Kangning Liu_files/UVIT_HR_multimodal.jpeg" alt="Publication Image for Unsupervised Multimodal Video-to-Video Translation" class="img-fluid publication-img" style="width: 100%;">
                            <!--<img src="./Kangning Liu_files/UVIT_overview.png" alt="Publication Image for Unsupervised Multimodal Video-to-Video Translation" class="img-fluid publication-img" style="width: 100%;">-->
                        </div>
                        <div class="custom-col-text" style="padding: 10px;">
                            <p><b>Unsupervised Multimodal Video-to-Video Translation via Self-Supervised Learning</b></p>
                            <p>
                                <b>
                                    [<a href="https://openaccess.thecvf.com/content/WACV2021/papers/Liu_Unsupervised_Multimodal_Video-to-Video_Translation_via_Self-Supervised_Learning_WACV_2021_paper.pdf">Paper Link</a>]
                                    [<a href="https://uvit.netlify.app">Project Link</a>]
                                </b>
                            </p>
                            <p><u><b>Kangning Liu*</b></u>, Shuhang Gu*<u>(*Equal contribution)</u>, Andrs Romero, and Radu Timofte</p>
                            <p>[WACV 2021]</p>
                            <p class="justified-text">
                                In this project, we introduce an unsupervised video-to-video translation model that decouples style and content.
                                Leveraging specialized encoder-decoder and bidirectional RNNs, our model excels in propagating inter-frame details.
                                This architecture enables style-consistent translations and offers a user-friendly interface for cross-modality conversion.
                                We also implement a self-supervised training approach using a novel video interpolation loss that captures sequence-based temporal information.
                            </p>
                        </div>
                    </div>
                </li>
                <!-- End of the paper -->
                <!-- Start of the paper: An interpretable classifier for high-resolution breast cancer screening images -->
                <li class="publication-item">
                    <div class="row">
                        <div class="custom-col-img" style="padding: 10px;">
                            <img src="./Kangning Liu_files/gmic.jpeg" alt="Publication Image for An interpretable classifier for high-resolution breast cancer screening images" class="img-fluid publication-img" style="width: 100%;">
                        </div>
                        <div class="custom-col-text" style="padding: 10px;">
                            <p><b>An interpretable classifier for high-resolution breast cancer screening images </b></p>
                            <p>
                                <b>
                                    [<a href="https://www.sciencedirect.com/science/article/pii/S1361841520302723">Paper Link</a>]
                                </b>
                            </p>
                            <p>Shen, Yiqiu, Nan Wu, Jason Phang, Jungkyu Park, <u><b>Kangning Liu</b></u>, Sudarshini Tyagi, Laura Heacock et al.</p>
                            <p>[Medical image analysis 2021]</p>
                            <p class="justified-text">
                                Medical images differ from natural images in significantly higher resolutions and smaller regions of interest.
                                Because of these differences, neural network architectures that work well for natural images might not be applicable to medical image analysis.
                                In this work, we propose a novel neural network model to address these unique properties of medical images.
                            </p>
                        </div>
                    </div>
                </li>
                <!-- End of the paper -->

            </ul>

        </div>

        <div class="col-md-13">
            <h2 id="teaching">Teaching</h2>
            <ul class="vert">
                <li>
                    <p><b>Probability and Statistics for Data Science, NYU, Center for Data Science</b></p>
                    <p>Teaching Assistant (Sept - Dec 2021)</p>
                </li>
                <li>
                    <p><b>Advanced Machine Learning, ETH Zurich, Department of Computer Science</b></p>
                    <p>Teaching Assistant (Sept - Dec 2018)</p>
                </li>
            </ul>

        </div>



        <div class="col-md-13">
            <h2 id="service">Service</h2>
            <ul class="vert">
                <li>
                    <p>Reviewer for CVPR (2022, 2023, 2024), ICCV (2023), NeurIPS (2022, 2023), AISTATS (2024)，AAAI (2023, 2024), WACV (2021, 2022, 2023) and MIDL (2022, 2023)</p>
                </li>
            </ul>

        </div>


        <h2 id="photography">Miscellaneous</h2>

        <!-- Your existing content here -->
        <ul class="vert">
            <li>
                <p>I am just starting out with Lightroom Magic using my Sony A7M4:)</p>
            </li>
        </ul>
        <br />
<div class="col-md-13">
    <p class="imgGallery-section-title">California Style</p>
    <div class="imgGallery-row">
        <div class="imgGallery-column">
            <img src="./Kangning Liu_files/DSC04444.jpeg" alt="California">
        </div>
        <div class="imgGallery-column">
            <img src="./Kangning Liu_files/DSC04257.jpeg" alt="California">
        </div>
        <div class="imgGallery-column">
            <img src="./Kangning Liu_files/DSC04158-2.jpeg" alt="California">
        </div>
    </div>
    <br />

    <p class="imgGallery-section-title">NYC Cityscapes</p>
    <div class="imgGallery-row">
        <div class="imgGallery-column">
            <img src="./Kangning Liu_files/8V4B0383.jpg" alt="NYC">
        </div>
        <div class="imgGallery-column">
            <img src="./Kangning Liu_files/8V4B0444-2.jpg" alt="NYC">
        </div>
        <div class="imgGallery-column">
            <img src="./Kangning Liu_files/DSC04963-4.jpg" alt="NYC">
        </div>
    </div>
    <br />

    <p class="imgGallery-section-title">Light and Night</p>
    <div class="imgGallery-row">
        <div class="imgGallery-column">
            <img src="./Kangning Liu_files/DSC09256.jpeg" alt="SteelRacks">
        </div>
        <div class="imgGallery-column">
            <img src="./Kangning Liu_files/DSC09385.jpeg" alt="SteelRacks">
        </div>
        <div class="imgGallery-column">
            <img src="./Kangning Liu_files/DSC09265.jpeg" alt="SteelRacks">
        </div>
    </div>
    <br />

    <p class="imgGallery-section-title">SteelStacks</p>
    <div class="imgGallery-row">
        <div class="imgGallery-column">
            <img src="./Kangning Liu_files/DSC06520-3.jpeg" alt="SteelRacks">
        </div>
        <div class="imgGallery-column">
            <img src="./Kangning Liu_files/DSC06601-2.jpeg" alt="SteelRacks">
        </div>
        <div class="imgGallery-column">
            <img src="./Kangning Liu_files/DSC06482-4.jpeg" alt="SteelRacks">
        </div>
    </div>
    <br />

    <p class="imgGallery-section-title">Vermont Fall</p>
    <div class="imgGallery-row">
        <div class="imgGallery-column">
            <img src="./Kangning Liu_files/DSC05394.jpg" alt="Vermont">
        </div>
        <div class="imgGallery-column">
            <img src="./Kangning Liu_files/DSC05554-2.jpg" alt="Vermont">
        </div>
        <div class="imgGallery-column">
            <img src="./Kangning Liu_files/DSC05782.jpg" alt="Vermont">
        </div>
    </div>
</div>



        <!-- /.container -->
        <!-- Other people may like it too! -->
        <!--    <a style="color:#b5bec9;font-size:0.8em; float:right;" href="https://github.com/mavroudisv/plain-academic">Plain Academic</a> -->

        <br />

        <br />
        <ul class="vert">
            <li>
                <p>Pageviews</p>
            </li>
        </ul>

        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=nk2suWeO4QvLQb3OSDKbWMWpp_rc2lrGUiGq_rrnFQg&cl=ffffff&w=a"></script>
    </div>

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>

